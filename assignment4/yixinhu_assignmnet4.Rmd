---
title: "R Notebook"
output: html_notebook
---

### Question 1:

Build an OLS model on your cleaned version of houses_training_set. And then, making whatever adjustments you deem appropriate, provide your best prediction of the sales price for each house in houses_test_1. Describe your thought process and modeling choices.
(Answer submissions should be contained in your .csv file with the first column being "parcel_number", the second being "sale_price", and the third being "question_1".)

### Response:

First, loading the data:

```{r}
library(ggplot2)
library(dplyr)
library(tidyverse)

training_raw <- read_csv("houses_training_set.csv") %>% 
  select(-X1)
test1_raw <- read_csv("houses_test_set_1_new.csv") %>% 
  select(-X1_1)
test2_raw <- read_csv("houses_test_set_2_new.csv") %>% 
  select(-X1_1)
```

Getting to know the data:

Notes:

  + 3 levels range from 1=poor to 3=good
  + cabinet and plumbing separates into classes out of 99 
    
  + Parcel number should be unique
  + sale_price shouldnt be negative 
  + Lots of characteristic variables that don't make sense to add:
    + floor_type and ceiling_type
  + Absurd quantities in:
    + t2nd_floor_area
    + school_values
    + total full bath

Deed restrictions help to safeguard the long-term value to the community of the initial investment in affordable homeownership by limiting any subsequent sales of the home to income-eligible borrowers at an affordable price. The resale restrictions are attached to the propertyâ€™s deed, and may be enforced for several decades or more, depending on state law. Deed restrictions typically prescribe a formula which sets a ceiling on the subsequent sales price of the home. As one example, the resale formula may be tied to changes in the area median income (https://www.localhousingsolutions.org/act/housing-policy-library/deed-restricted-homeownership-overview/deed-restricted-homeownership/)


```{r}
summary(test2_raw)

unique(test2_raw$fuel_storage_proximity)

training_raw %>% 
  ggplot(aes(x=lot_size)) +
  geom_histogram()

training_raw %>% 
  filter(lot_size>50000)

training_raw %>% 
  count(parcel_number)

training_raw %>% 
  filter(parcel_number == 71035206210)
```
It is worth noting that there is duplicate parcel_numbers values. Since I believe that a parcel number should be a unique identifier for property, assuming that the fact that there later sale dates for the same parcel number generally has an increasing price and that prices trend upwards over time for real estate, I believe that the most recent price would be must suitable for estimation as it already encapsulates past information. Therefore, I will filter out the past dates and only keep the most recent prices for a specific parcel number.

From looking at the basic summary of all the datasets, I found that all the school number data have 999s for values. I do not believe that there is a reasonable explanation for this value, so in avoiding introducing my own biases into the data, I will go ahead and remove these values from the dataframes. To do this, I find that there is a huge gap between the number of reasonable values and the 999s, therefore, I will take a value cutoff at 900 to make the filter out this extreme outlier.

Other absurd values lie in assessment area, where in all the data sets, there is a value of 6000+ as well as a 11 rows consisting of assessment_area of 5000+. While I believe that the removal of one single point in a data set consisting of 14500 observations initially will not distort our results, an argument can definitely be made that this one singular value has a missing decimal point. I would believe that this is the case, and include decimal points to all values that are significantly greater than the cluster of distributions around 100. I will use the 500 mark as a cut off, since the values in question are all 5000+, and include the missing decimal point. 

Specific to the training data, we see that there are 50 counts with negative 99999s as well as 23 values of 0. I don't think that there is a valid explanation besides this was encoding missing data, therefore, I will remove these values in order to make sense of the data as a whole. There is also a quick item to note, which is that there are multiple columns where sale_price is 2600, 24 to be exact. However, other values in the data shows that they do in fact vary with respect to different characteristics, so I do not believe that they are necessarily duplicates.

Looking at lot size, I see that there is an extreme outlive in the data at 16000+ sq feet within the training data, as well as multiple observations above 50000 for the 2nd testing data. For a home that is under $300,000, it would be extremely unlikely to have a yard that is more that 3 football fields large. Notice then that this causes our data to be skewed with a long right tail, and thus these outliers will have a significant impact on our predictive ability. Therefore I would argue that these outliers which lie beyond the 97.5 percentile above the mean should be removed. Furthermore, there is a very interesting part of the data which shows that the lot sizes are bimodally distributed, where it seems that there are two normal distributions stacked on top each other. This will require further diving into

For the 2nd testing set. It is *possible* that there is some kind of mega mansion where the yard is huge, so removing them outright would be unsatisfactory. However, I would argue that even if this is the case, these values, amounting to 18 observations in total, will bring significant predictive contributions as a testing data set due to them being quite extreme outliers. Therefore, I have provided an argument which is to say that these 18 observations are quite extreme outliers in the testing data number 2, and while being different in sampling than the training and testing data 1, it is still too ridiculous of a value to have for testing purposes. 

In the training data set, we have values of ground floors being 1 sqft, which is flat out ridiculous, as well as -999 for second floor areas, to be concrete, there is 1 value which does not make sense. In the test data 1, there are 4 entries that are evidently outliers which are removed from the distribution, and thus I will remove them accordingly to make a better overall prediction. Because I cannot make sense of this information (typos, different units etc.), I will remove it from the dataset. Notice that the zero values of additional levels (2+) are not of concern. I believe that this information encodes the fact that these properties do not have an additional levels.

Now looking at street noise, railroad noise, and airport noise. I think it would be helpful to not only look at the training data frame by itself, but also in conjunction with testing data 2, since I would suspect that an increase in street noise would likely mean that the property is situated in a busy urban location, thus usually this would mean higher property prices. However, It seems that for all dataframes, the values are either 0 or 60 (61 or street and railroad noise). This really does not help at all with my thinking because the values are quite undecipherable. To avoid adding unsound arguments, I will forgo these variables for now.

At this point, the main data cleaning is tentatively complete, where we actually see that most variables between test 1 and training share similar distributions (which is something that we expect). I will move on to the regression construction and conduct further cleaning should the issues arise.

```{r}
### CHECK PARCEL_NUMBER

temp_train <- training_raw %>% 
  group_by(parcel_number) %>% 
  filter(sale_date == max(sale_date)) %>% 
  ungroup()

### REMOVING -99999 AND 0 FROM TRAINING DATA SALES PRICE

temp_train <- temp_train %>% 
  filter(sale_price>0)

### FILTERING PROPERTY AREAS

temp_train <- temp_train %>% 
  mutate(t2nd_floor_area = ifelse(t2nd_floor_area<0,0,t2nd_floor_area)) %>% 
  filter(ground_floor_area>10)

### CHECKING BATHROOM DISTR

temp_train <- temp_train %>% 
  filter(total_full_bath>=0)

### CHANGING INTO FACTORS

temp_train <- temp_train %>% 
  mutate(assessment_area = as.factor(assessment_area),
         deed_restrictions = as.factor(deed_restrictions),
         national_historic_district = as.factor(national_historic_district),
         street_noise = as.factor(street_noise),
         high_school_number = as.factor(high_school_number))  

temp_test1 <- test1_raw %>% 
  mutate(assessment_area = as.factor(assessment_area),
         deed_restrictions = as.factor(deed_restrictions),
         national_historic_district = as.factor(national_historic_district),
         street_noise = as.factor(street_noise),
         high_school_number = as.factor(high_school_number))  

temp_test2 <- test2_raw %>% 
  mutate(assessment_area = as.factor(assessment_area),
         deed_restrictions = as.factor(deed_restrictions),
         national_historic_district = as.factor(national_historic_district),
         street_noise = as.factor(street_noise),
         high_school_number = as.factor(high_school_number))  

### TRNCATE OUTLIERS AND CREATING VALIDATION AND TRUE TRAINING SET

set.seed(12)

temp_train <- temp_train %>% 
  filter(sale_price >= quantile(temp_train$sale_price,0.02))

smpl_size <- floor(0.8 * nrow(temp_train))

train_idx <- sample(seq_len(nrow(temp_train)), size = smpl_size)

true_train <- temp_train[train_idx,]
validation <- temp_train[-train_idx,]

# ### TRUNCATE OUTLIERS
# 
# temp_train %>% 
#   ungroup() %>% 
#   summarize(max_price = max(sale_price),
#             min_price = min(sale_price))
# 
# unique(temp_train$sale_price)

```
It is worth noting that I have also included an interaction 

For the actual model, I believe that there are few variables that are truly significant in determining something as crucial as a real estate's value. In particular, instead of the granular information on how many windows it has, I believe that assessment area and living area (i.e. area of floors) are the main "intrinsic" properties which determine the price of a real estate. 

However, that is not the whole story: after learning about how real estate is conventionally sold, it is the case that square footage is not the only fundamental determinant of prices, rather, increased "functionality" (i.e. bedrooms, washrooms etc.) are also extremely important to determine how 'valuable' is a piece of real estate. Lastly, it also should be the case that all else equal, a well-maintained home should be more "valuable" than a run-down home, hence I believe that conditions of the home should also be a factor when considering housing price as well as when the property is constructed. Therefore, I will use area, functionality, and quality to map out the "intrinsic" properties of a piece of real estate. (other bells and whistles such as additional luxuries, pools for example, will be considered later when estimating prices for luxury homes in the second test)

Beyond "intrinsic" properties, and perhaps more importantly, the location of the real estate is extremely important. 

It is the case that you can have an extremely luxurious mansion, but if its stuck in a sketchy neighborhood far from where you work, the willingness to pay for the property might not actually be as high as you would have expected. Furthermore while it would be helpful to get data such as local crime rates, mean incomes, and traffic to further estimate the environment/neighborhood of the property, we simply do not have that full set of data, and we are stuck with whatever "external" data that we have. In particular, I believe that number of schools as a starting point could be helpful in measuring the environment of the neighborhood. This is because access to education often signifies increased school spending in that area, and as research conducted by Barrow et al. (https://www.nber.org/digest/jan03/school-spending-raises-property-values), has a positive effect on per pupil housing values. Furthermore, schools can also serve as a proxy for community stability, as often times amenities (such as basic infrastructure like sewers and clean water, and as well as policing) will be supplied to schools, which should have a positive effect on real estate prices relative to those in less stable communities. (If we also had data on the *quality* of the school rather than the quantity, our estimates would be better as per the Brookings Institute: https://www.brookings.edu/research/housing-costs-zoning-and-access-to-high-scoring-schools/).

Lastly, I believe that housing in "special districts" will also be quite important to consider, as it is usually the case that areas with historical significance will positively contribute to sales prices (such as UPC in Hyde Park, where the facilities are not the best, but still cost quite a lot due to it being designed by the same person who designed the Leo),or other 

But nevertheless, let us look at the entire data frame first for the sake of completion 

```{r}
library(corrplot)
library(RColorBrewer)

M_df <- temp_train %>% 
  select(-deed_restrictions,-national_historic_district,-flood_plain,-fuel_storage_proximity,-parcel_number) %>% 
  mutate_if(is.character,as.factor) %>% 
  mutate_if(is.factor,as.numeric)

corr_plain <- cor(M_df)
corr_plain[lower.tri(corr_plain,diag=TRUE)] <- NA
corr_plain[corr_plain == 1] <- NA

corr_plain <- as.data.frame(as.table(corr_plain))

corr_plain <- corr_plain %>% 
  filter(abs(Freq)>0.3) %>% 
  arrange()

M_corr_df <- reshape2::acast(corr_plain,Var1~Var2,value.var="Freq")

corrplot(M_corr_df, is.corr=FALSE, tl.col="black", na.label=" ",col=brewer.pal(n=8, name="RdYlBu"))
         
```

The graph is kinda hideous, so for now lets just run with the regression:

The first regression will start out simple, I will only consider the main components stated above, i.e. the intrinsic size of the property (areas) and access to external amenities (proxy by schools). 

```{r}

ass_fixeff_lm <- 
  lm(data = true_train, sale_price ~ ground_floor_area + t2nd_floor_area + t3rd_floor_area + finished_basement_area + finished_attic_area + additions_area + total_full_bath + total_half_bath + total_dens + total_kitchens + total_dining + amp_rating + sale_date + stalls_garage_1 + stalls_garage_2 + quality_class + inside_condition + outside_condition + plumbing_class + central_air + national_historic_district + street_noise + high_school_number)

summary(ass_fixeff_lm)

## consider quality, year, 
```

Here, we have quite a significant and intuitive result when looking at the model at hand. For each square ft increase in assessment area and living area (floor areas), we see a statistically significant increase in housing price. Furthermore, we see that this number makes economic sense as well, as a quick Google search shows that in the Midwest, average cost per square foot is around \$106.79. This value is sure to be lower in our analysis because we have added additional covariates to explain real estate prices. A quick note is that we see additional floors are generally more expensive per square ft than the ground floor, which also makes intuitive sense. The only interesting finding is that dining rooms are insignificantly different from zero, with a point estimate of negative $\$1277$ and S.E. of $\$6445$. This is quite surprising and does not make any intuitive sense what so ever, as we would expect housing with no kitchens to be worth much less than housing with at least one kitchen. My explanation for this could be the fact that almost every single entry has at least 1 kitchen, and there are only 32 out of 13663 entries in the cleaned training data that does not have a kitchen value of 1. Therefore, it could very likely be the case that the "unpredictiveness" of the kitchen variable is due to the fact that practically every single entry has at exactly the same value. Lastly, it should be noted that price is also positively increasing with respect to construction year, which makes sense since more recent property are generally in a better condition than property that has been standing for 100 years, regardless of how many "renovations" that had been conducted. 

Now looking at the 'external' factors combined with square footage, I have selected the access to schools for this analysis. By looking at availability of schools, we see that middle schools and high schools have a positive and statistically significant result, where access to one additional middle school raises prices by $\$79.26$, where as high school raises prices by a (quite large) value of around $\$3743$. Furthermore, as expected, we see that property in a national historic district has a positive effect on price by a factor of $\$35850$. In the end, this model has an $R^2$ of around 0.7697. 

Now onto a predicting sample with the "validation" set

```{r}
validation$ols_pred <- predict(ass_fixeff_lm, validation)
true_train$ols_pred <- predict(ass_fixeff_lm, true_train)

validation <- validation %>% 
  group_by(parcel_number) %>% 
  mutate(error = ols_pred-sale_price) %>% 
  arrange(-desc(abs(error))) %>% 
  ungroup()

validation %>% 
  summarize(mse=(mean(error^2)))

true_train %>% 
  summarize(mse=(mean((ols_pred - sale_price)^2)))
  

extreme_overshot_parcels<- as.vector(c(validation %>% 
  filter(ols_pred>175000 & sale_price<100000) %>% 
  summarize(parcel_number=parcel_number)))

comp <- c(71034106023,71034102055)
full <- c(60803103188,60803104087,60803105176,60811107106,60811112197,71011112077,71034106023,71035205098,71035206210,71035208026,71035208034,71034102055)

validation %>%
  filter(parcel_number %in% comp) %>%
  relocate(sale_price,ols_pred,error,)
val_lm <- lm(data=validation, sale_price ~ ols_pred)

summary(val_lm)

validation %>% 
  ggplot(aes(x=sale_price,y=ols_pred)) +
  geom_point()

validation %>% 
  ggplot(aes(x=ols_pred))+
  geom_density()

# validation <- validation %>% 
#   mutate(ols_pred_scaled = mean(ols_pred)) %>% 
#   mutate(ols_pred_scaled = ols_pred_scaled*0.5 + mean(ols_pred))

# validation %>% 
#   summarize(rmse_scaled=sqrt(mean((sale_price - ols_pred_scaled)^2)))


```

In looking through our validations, there are a few odd pieces of data that is quite interesting. First of all, a regression between the true price and the predicted price has a coefficient point estimate on the prediction of 0.989, which means that our predictions itself does not overestimating or underestimating the prices systematically. Particular for our validation set, after we have tuned the model, we have obtained a MSE of 499866765, and the distribution of the estimated prices are also given below as a sanity check (see if there are extreme values in our predictions). Finally, we move on to the predictions of the first testing set.

```{r}
temp_test1$ols_pred <- predict(ass_fixeff_lm,temp_test1)

q1_otpt <- temp_test1 %>% 
  select(parcel_number, ols_pred)

```

The estimations given for the testing dataset is exported as "XXXXXXXXXXXXXXXXXXX.csv".

### Question 2

Build a RANDOM FOREST model on your cleaned version of houses_training_set. And then, making whatever adjustments you see fit, provide your best prediction of the sales price for each house in houses_test_1. Describe your thought process and modeling choices. 

### Response:

Building the random forest model, we will use whatever insight that was obtained previously to proceed with the model. Before we start, it is worthwhile to note that random forest models inherently has variable selection, which to some extent removes the requirement for my own judgment. However, I think it would not be idea if we just let the model do whatever it wants. To address this fully, I will experiment with different model specifications and provide the most sensable model as my final result. In doing so, I am able to (at least) get some sense as to what is going on behind the scenes and comment on the way.

```{r}
library(randomForest)
library(reshape2)
# 
# true_train <- true_train %>% 
#   mutate(street_noise = as.integer(street_noise),
#          street_noise = ifelse(street_noise>0,1,0),
#          national_historic_district = as.integer(national_historic_district),
#          national_historic_district = ifelse(national_historic_district>0,1,0),
#          high_school_number = as.factor(high_school_number))

# validation <- validation %>% 
#   mutate(street_noise = as.integer(street_noise),
#          street_noise = ifelse(street_noise>0,1,0),
#          national_historic_district = as.integer(national_historic_district),
#          national_historic_district = ifelse(national_historic_district>0,1,0),
#          high_school_number = as.factor(high_school_number))

rf_model <- randomForest(data = true_train, sale_price ~ ground_floor_area + t2nd_floor_area + t3rd_floor_area + finished_basement_area + finished_attic_area + additions_area + total_full_bath + total_half_bath + total_dens + total_kitchens + total_dining + amp_rating + sale_date + stalls_garage_1 + stalls_garage_2 + quality_class + inside_condition + outside_condition + plumbing_class + central_air + national_historic_district + street_noise + high_school_number, mtry = 7, importance = TRUE)

summary(rf_model)

validation$rf_pred <- predict(rf_model,validation)

validation %>% 
  mutate(error_rf = rf_pred - sale_price) %>% 
  relocate(parcel_number,sale_price,ols_pred,rf_pred,error,error_rf) %>% 
  summarize(mse=(mean(error^2)),
            mse_rf=(mean(error_rf^2)))

val_lm_rf <- lm(data=validation,sale_price ~ rf_pred)
summary(val_lm_rf)

validation %>% 
  ggplot(aes(x=sale_price,y=rf_pred)) +
  geom_point()

validation %>% 
  ggplot(aes(x=sale_price,y=ols_pred)) +
  geom_point()

graphing <- melt(validation, id.vars='parcel_number',variable.name='series')

validation %>% 
  ggplot(aes(x=ols_pred)) +
  geom_density() +
  labs(title="OLS PREDICTION VALIDATION")

validation %>% 
  ggplot(aes(x=rf_pred)) +
  geom_density() +
  labs(title="RF PREDICTION VALIDATION")

validation %>% 
  ggplot(aes(x=sale_price)) +
  geom_density() +
  labs(title="TRUE SALES")

validation %>% 
  summarize(max_price=max(ols_pred),
            min_price=min(ols_pred))
```

After tuning the model, we have arrived at a good MSE value for RF at mtry = 7. Now, we can make a clear comparison between the MSE of OLS estimates and MSE of RF estimates from the same true testing set and the validation set. In particular, we have MSE of OLS = 499866765, whereas the MSE of RF = 422754957 This shows that the RF performs better than the OLS when testing against our validation set. It is of course important to note that these values will differ from different samplings of true training sets and validation sets. However, it is a good way to measure how well we are doing with the models and tune them accordingly. Now, to make the predictions for the first set of testing data using the RF model


```{r}
temp_test1$rf_pred <- predict(rf_model,temp_test1)

temp_test1 %>% 
  ggplot(aes(x=rf_pred)) +
  geom_density()

temp_test1 %>% 
  ggplot(aes(x=ols_pred)) +
  geom_density()

validation %>% 
  ggplot(aes(x=rf_pred)) +
  geom_density()

validation %>% 
  ggplot(aes(x=ols_pred)) +
  geom_density()

q2_otpt <- temp_test1 %>% 
  select(parcel_number,rf_pred)
```

The final file is given as "XXXXXXXXXX.csv"

### QUESTION 3:

Using whatever modeling approach and whatever adjustments you deem appropriate, provide your best prediction of the sales price in houses_test_2.  Describe your thought process and modeling choices. NOTE: you have to be thoughtful here!! 

### Response:

There are two things that we have to consider when doing this part. 

  + $300,000+ Sale price homes are now included
  
  + 2 or fewer full bathrooms are now included
  
Therefore, we must somehow take into consideration how these two factors will affect our estimations. First checking the distribution of full bathrooms vs. the training set...

```{r}

temp_test2 %>% 
  ggplot(aes(x=total_full_bath)) +
  geom_histogram()

temp_train %>% 
  ggplot(aes(x=total_full_bath)) +
  geom_histogram()

temp_test2 %>% 
  ggplot(aes(x=high_school_number)) +
  geom_bar()

temp_train %>% 
  ggplot(aes(x=high_school_number)) +
  geom_bar()

temp_train %>% 
  ggplot(aes(x=national_historic_district)) +
  geom_bar()

temp_test2 %>% 
  ggplot(aes(x=national_historic_district)) +
  geom_bar()

temp_train %>% 
  ggplot(aes(x=sale_date)) +
  geom_histogram()

temp_test2 %>% 
  ggplot(aes(x=sale_date)) +
  geom_histogram()

unique(temp_test2$street_noise)
```

Indeed, we see that most of the full bathroom in the second testing set is centered around 3 bathrooms, which we cannot estimate directly from the training set. However, recall that in our OLS method, we did not use this data as a factor (fixed effect), and treated it linearly. This makes sense because it is common in the real estate circle to use the amount of full bathrooms to estimate how luxurious a property is (if a smaller bathroom will do this trick, having a full bathroom is simply for luxury), therefore, the OLS method seems to have a better chance at predicting correctly relative to RF methods. Furthermore, since we know that the second testing set has either more bathrooms OR has its value at $300,000, we can almost say for certain that there will be necessity to tune the OLS model to account for the higher-price housing. 
  
Exploring the data of our indicated set of variables supports our "luxury" assumption. By checking the key groups of factors: intrinsic and external, we see that distribution of area of property in the 2nd testing set is shifted to the right and has a thicker right tail, indicating an increase in larger homes. They have more garages, increased quality class, higher AMP ratings. However, at the same time test set 2 has similar inside and outside conditions, similar central air, and similar sale dates. However, I think that to expand and correctly estimate luxury homes, we should include more variables considering "non-essential" characteristics, such as fireplace openings.

Essentially, it is as if we are blind to the "luxurious data" and we are estimating their property using only information that we see in our normal lives. This intuitively means that what might be considered "valuable" for us, such as good plumbing, might be completely an after thought in higher-end housing since it is already become a "given" for them. It is **not** the case that they pay "less" for the basic amenities, but it should be the case that the *bulk of their value comes from somewhere else* that we do not observe clearly in our "non-luxurious" data. Therefore, if we are to only look at our training data, it is almost for sure that we will be underestimating their housing prices, since it is as if "good pluming" is the limit of determining value and completely oblivious to the value given by, say, a backyard tennis court. In this light, I think a good way to account for this shift is to make a "shift" parameter which adjusts our biases in the predictions **OVERALL**.

On the other hand, we have cases where housing with 2 or less bathrooms are included in the data, which means that these must be priced \$300,000 or *HIGHER*. On the other hand, we could also possibly have cases where properties have high full bathrooms, but possibly priced *LOWER* than \$300,000. This would mean that our full bathroom point estimate is quite unreliable. While it is true that in our previous testing that full bathroom is a great predictor of price, our new data is definitely going to skew the point estimate on the full bathroom variable. In the case where we have introduced housing that has \$300,000 price and yet lower full bathroom counts, this would mean that our predictor of full bathroom would increase, whereas the case where we have introduced housing that has less than \$300,000 but higher full bathrooms, the predictor of full bathroom would decrease. 

While one possible way to address this problem is to lump the two variables together. However, since I have checked that the two variables are independent (by checking their interaction terms), lumping the two variables into a joint "bathroom" effect would not be a sound procedure as we are basically saying two full bathrooms have the same effect on price as two half bathrooms.

Therefore, the best thing that we can do model wise is to work with our OLS assumptions that even in the situations where we have out-of-bounds data, our growth is expected to be linear in nature. However, there are much more things we can do to our training data to make sure that the training data distributions fits the testing data as much as possible, and thinking of a good way to measure the effect of full baths without directly using the variable. (use the proportion of full baths in a house?)

To do this, I will split the training data into two segments: "high-cost" housing and "normal" housing. This is to replicate the situation we have here with test set 2. Then I will list a bunch of assumptions to make our logic consistent with our model. After constructing our model, I will go ahead and create the shift parameter which measures our inherent bias in predictions, and use this constructed parameter as a hyper parameter (i.e. something we choose ourselves) to tune our predictions accordingly.

Therefore, we need to find a subset of the training set to mimic the second testing data as much as possible, so that we are using a representative (as possible) sample to train our model.

```{r}
# FINDING A "SIMILAR" DISTRIBUTION OF CHARACTERISTICS 


q50 <- quantile(temp_train$sale_price,0.5)
q70 <- quantile(temp_train$sale_price,0.7)

q3_test <- temp_train %>% 
  filter(sale_price>200000)

q3_train <- temp_train %>% 
  filter(sale_price<200000)

temp_train %>% 
  group_by(total_full_bath) %>% 
  summarize(mean=mean(sale_price),
         sd=sd(sale_price))

192785.0/154735.3
43986.84/41138.27

temp_train %>% 
  filter(total_full_bath==2) %>% 
  ggplot(aes(x=sale_price)) +
  geom_histogram()

gp1 <- temp_train %>% 
  filter(sale_price>q70) %>%
  select(parcel_number,total_full_bath,plumbing_class)
  
gp1 <- melt(gp1,id.vars="parcel_number")

gp1 %>% 
  ggplot(aes(x=value,color=variable)) +
  geom_histogram(position="dodge")

gp2 <- temp_test2 %>% 
  select(parcel_number,total_full_bath,plumbing_class)
  
gp2 <- melt(gp2,id.vars="parcel_number")

gp2 %>% 
  ggplot(aes(x=value,color=variable)) +
  geom_histogram(position="dodge")

temp_train %>% 
  filter(plumbing_class>6) %>% 
  ggplot(aes(x=total_full_bath)) +
  geom_bar()

temp_test2 %>% 
  filter(plumbing_class>0) %>% 
  ggplot(aes(x=total_full_bath)) +
  geom_bar()

temp_train %>% 
  group_by(total_full_bath) %>% 
  summarize(mean = mean(sale_price),
            sd = sd(sale_price),
            n = n()) %>% 
  ungroup() %>% 
  mutate(ratio_mean = 193183.7/157082.4,
         ratio_sd = 43594.70/39940.12)

temp_test2 %>% 
  ggplot(aes(x=total_full_bath)) +
  geom_histogram()

temp_train %>% 
  ggplot(aes(x=ground_floor_area)) +
  geom_density()

temp_test2 %>% 
  ggplot(aes(x=ground_floor_area)) +
  geom_density()
```

Here, we have taken an arbitrary cut off to mimic the situation where we must use the full training data to approximate the testing set 2. Doing so allows us to investigate a few aspects of our model which will be useful in approximating. Specifically, we are told that our data is *biased* in terms of pricing, as well as the variable on full bathrooms are *out-of-bounds* in the testing data relative to our training data. Therefore, the aspects that we are trying to estimate in our "mock test" is:

  + to what degree are the estimates biased in terms of sales price on a whole, and thus coming up with a so called "shift parameter"
  
  + to what extent do increased full bathrooms change the distribution in sale prices, and thus coming up with a "distribution multipliers" for each extra full bathroom
  
Needless to say, we have some key assumptions when using these values. In tuning the model, we are making the following assumptions:

  + the training data set's properties are "less-luxurious" than the second testing set's data in terms of prices and full bathrooms
  + between the two data sets, point estimates on fundamental characteristics are still relatively the same (i.e. going from 500 sqft to 501 sqft increases the value of any property by a similar amount, all else equal, between the two sets of data)
  + **the bias caused by predicting "more-luxurious" housing and "less-luxurious" housing is relatively consistent across groups of property with similar characteristics (shift parameter)**
  + **additional functionality provided by additional bathrooms contribute to a change in prices by a relatively consistent ratio in terms of distributions' mean and standard deviations (distribution multipliers)**

Running some tests in our "mock test", we find that our OLS estimates using the training data below the arbitrary cut offs is systematically underestimating the prices, where the true prices are on average 1.2x times higher than our predicted results for multiple cutoffs, meaning that our shift parameter should be around 1.2x. Looking at the distributions of prices between 1 full bathroom vs 2 full bathrooms, we find that the mean of 2 full bathrooms is 1.25x higher than the mean for 1 full bathroom, and the standard deviation is 1.07x higher in terms of standard deviation, meaning that these two values will provide the distribution multipliers 1.25 for the mean and 1.07 for the standard deviation. However, it is also worth noting that even after all this tuning, if we find cases where full bathrooms < 2 AND sale price < /$300,000, it would mean that we would of course be incorrect since this data would have never even appeared in our testing set 2. Therefore, I will set these values to \$300,000 as it is the lowest (and thus closest value to our estimation) value which satisfies the conditions on our test set 2.

Therefore, we will be using these parameters to first estimate the entire testing set 2 using OLS. Then, I will consider the overall bias and shift the predicted prices by 1.2x. Then, I will have to consider the distributions of prices for each amount of full bathrooms to see if anything is needed in terms of the distribution, and apply our distribution multipliers accordingly. Finally, I will consider the items where it would have never existed in the testing set 2 and rescale their estimations to /$300,000 to comply with the data description

```{r}
test2_raw %>%
  mutate(total_bath = total_full_bath + total_half_bath) %>% 
  ggplot(aes(x=total_bath)) +
  geom_histogram()

temp_train %>% 
  mutate(total_bath = total_full_bath + total_half_bath) %>% 
  ggplot(aes(x=total_bath)) +
  geom_histogram()

## AGGREGATING FULL BATHROOM DATA

test_q3_lm <-
  lm(data = q3_train, sale_price ~ ground_floor_area + t2nd_floor_area + t3rd_floor_area + finished_basement_area + finished_attic_area + additions_area + total_full_bath + total_half_bath + total_dens + total_kitchens + total_dining + amp_rating + sale_date + stalls_garage_1 + stalls_garage_2 + quality_class + + central_air + national_historic_district + street_noise + high_school_number + fireplace_openings + parcel_view + traffic + floor_types_1_and_2 + effective_yr)

q3_lm <-
  lm(data = temp_train, sale_price ~ ground_floor_area + t2nd_floor_area + t3rd_floor_area + finished_basement_area + finished_attic_area + additions_area + total_full_bath + total_half_bath + total_dens + total_kitchens + total_dining + amp_rating + sale_date + stalls_garage_1 + stalls_garage_2 + quality_class + + central_air + national_historic_district + street_noise + high_school_number + fireplace_openings + parcel_view + traffic + floor_types_1_and_2 + effective_yr)

test_all_q3 <-
  lm(data = q3_train, sale_price ~.)

summary(test_q3_lm)
summary(q3_lm)

q3_test$ols_pred <- predict(test_q3_lm,q3_test)
temp_test2$ols_pred <- predict(q3_lm,temp_test2)

val_lm_q3 <- lm(data=q3_test,sale_price ~ ols_pred)

temp_test2 %>% 
  ggplot(aes(x=ols_pred)) +
  geom_histogram()

summary(val_lm_q3)

q3_test %>% 
  ggplot(aes(x=sale_price,y=ols_pred)) +
  geom_point()

q3_test <- q3_test %>% 
  mutate(diff = sale_price-ols_pred) %>% 
  ungroup() %>% 
  mutate(mean_diff = mean(diff)) %>% 
  mutate(shift_param = mean_diff/mean(ols_pred)) %>% 
  relocate(parcel_number,sale_price,diff,mean_diff,shift_param) %>% 
  ##SHIFTING TO ADJUST FOR THE SYSTEMATIC BIAS IN OUR SAMPLE (1.2)
  mutate(shifted_pred = ols_pred + 0.2*ols_pred)#scaling to consider the unobserved factors of price 
  ##DO THE PERCENT SHIFT INCREMENTLY ON EACH SUB CATEGORY OF BATHROOMS (IN TESTING: 200,000)

  # mutate(fb2 = total_full_bath==2) %>%
  # group_by(fb2) %>%
  # mutate(mean_fb2 = mean(sale_price),
  #        sd_fb2 = sd(sale_price)) %>%
  # ungroup() %>%
  # mutate(normal_pred = ifelse(total_full_bath>=2,((((shifted_pred - mean_fb2)/sd_fb2)*45615)+mean_fb2),shifted_pred))

temp_test2_small <- temp_test2 %>% 
  mutate(shifted_pred = 1.2*ols_pred) %>% 
  relocate(parcel_number,ols_pred,shifted_pred) %>% 
  select(parcel_number,ols_pred,total_full_bath,shifted_pred)

temp_test2_small %>% 
  ggplot(aes(x=ols_pred,color=as.factor(total_full_bath))) +
  geom_density()

gpt2 <- melt(temp_test2,id.vars="parcel_number")

gpt2 %>% 
  ggplot(aes(x=value,color=variable)) +
  geom_density()

temp_test2_small %>% 
  group_by(total_full_bath) %>% 
  summarize(m=mean(shifted_pred),
            sd=sd(shifted_pred))

temp_test2_small <- temp_test2_small %>% 
  mutate(normal_price=case_when(total_full_bath==1 ~ shifted_pred,
                                total_full_bath==2 ~ shifted_pred,
                                total_full_bath==3 ~ (1.06*42538.72)*(shifted_pred - 283736.5)/61715.31 + (1.25*329909.8),
                                total_full_bath==4 ~ (1.06*61715.31)*(shifted_pred - 386751.3)/67979.43 + (1.25*283736.5),
                                total_full_bath==5 ~ (1.06*67979.43)*(shifted_pred - 453695.0)/50330.99 + (1.25*386751.3),
                                total_full_bath==6 ~ (shifted_pred - 503841.9) + (1.25*453695.0)))

temp_test2_small_gp <- temp_test2_small %>% 
  mutate(normal_price = ifelse(normal_price<300000 & total_full_bath<2,300000,normal_price)) %>% 
  select(parcel_number,ols_pred,shifted_pred,normal_price)

temp_test2_small %>% 
  ggplot(aes(x=normal_price,color=as.factor(total_full_bath))) +
  geom_density()

gpt3 <- melt(temp_test2_small_gp,id.vars="parcel_number")

gpt3 %>% 
  ggplot(aes(x=value,color=variable)) +
  geom_density()

temp_test2_small %>% 
  ggplot(aes(x=normal_price)) %>% 
  geom_density

graphing <- q3_test %>% 
  select(parcel_number,sale_price,ols_pred,low_bathroom_pred,shifted_pred)

graphing <- melt(graphing,id.vars="parcel_number")

graphing %>% 
  ggplot(aes(x=value,color=variable)) +
  geom_density()

q3_test %>% 
  ggplot(aes(x=sale_price,y=shifted_pred)) +
  geom_point()

val_lm_q3 <- lm(data=q3_test,sale_price ~ shifted_pred)

summary(val_lm_q3)

q3_test %>% 
  ungroup() %>% 
  summarize(mse_ols = mean(diff^2),
            mse_shft = mean((shifted_pred-sale_price)^2),
            mse_norm = mean((normal_pred-sale_price)^2),
            mse_rescaled = mean((rescaled_predictions-sale_price)^2))

746892718/1375386176

temp_train %>% 
  summarize(sd = sd(sale_price))
```
Notice that when we use our shift parameter, 
