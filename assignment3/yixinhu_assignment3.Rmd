---
title: "R Notebook"
output: html_notebook
---

### Q1: 

Without running any regressions, get to know the data. Describe interesting features you see in the data, especially with respect to the championship data and differences between the regular season and championship data.

### Response:

First loading the data:

```{r}
library(dplyr)
library(tidyverse)
library(stringr)
library(haven)
library(ggplot2)
library(rio)
library(tseries)
library(ggpubr)

regular_df <- read_dta("regular_seasons.dta")
champions_df <- read_dta("championships.dta")
qualifiers2020_df <- read_dta("2020_qualifiers.dta")

regular_df <- regular_df %>%
  mutate(year = case_when(season<76 ~ 2017,
                          season>=76 & season<80 ~ 2018,
                          season>=80 & season<84 ~ 2019,
                          season>=84 ~ 2020))
```

Now glimpsing at each of the data frames, we see that:
  + Qualifiers only contain ID of qualified individuals
  + regular data contains season and matches to indicate the time the questions were taken, whereas championship data only contains rounds to indicate when the questions were taken
  + in the regular data, individuals can take questions only belonging to a specific category, whereas the finals has no category whatsoever.
  + id and user_id seem to indicate the same entity (identification of individual) in the championship and regular data respectively, but with different names.
  
Furthermore, there are some interesting findings within each piece of data as well. For example, there are a total of 14 seasons in the regular season data. Since the data is from 2017-2020, we expect there to be 16 seasons. Therefore, one observation could be that (without looking at any other source) there is only data up to to mid-2020, and the last two seasons of 2020 was not observed, assuming that the data indeed starts on the first season of 2017. Therefore, I will treat the seasons' corresponding years accordingly. 
  
From here, recall that we are most interested on the differential outcomes between regular and championship seasons to tease out whether or not individuals were 'cheating' in the regular season. This is because the championships are live, and individuals cannot readily cheat in that context. Therefore, some basic visualizations will be used to explore the %correct between the two sets of data, as well as within each set of data (say, based on categories, seasons, or between matches). First, however, I will check the distributions of %correct based on categories, matches, and seasons to check if there is some persistence in the regular season data only. If there is some structural differences (i.e. for some reason, different categories have different distributions of mean correct rates), then it would be wise to take a closer look at these variables, and maybe control from them when building the models in the following questions. I will proceed in the following order: 
  + take a look at the aggregate (not looking at different groups) data in the regular season.
  + take a look at the aggregate (not looking at different groups) data in the final season.
  + based on previous observations, address the main issue of finding the differences between the champions and regular season data.
  
  
First, I will show the mean correct rate based on different seasons to see if different years will have different outcomes for the entire group (i.e. If we see that some seasons have significantly different correct rates than any other season, it would be a good idea to consider this factor when predicting within the regular season data.),
  

```{r}
regular_df %>%
   group_by(season) %>%
  summarize(mean_correct = mean(correct)) %>% 
  ggplot(aes(x=season,y=mean_correct)) +
  geom_line(color="black") +
  geom_point(shape=21, color="black", fill="#69b3a2", size=5) +
  labs(title="Mean correct rate of all individuals through the regular seasons") +
  theme(plot.title=element_text(face="bold",size=(13)))
```

We see that on aggregate (not distinguishing between subgroups), the mean correct rate varies with seasons as well. In particular, we have large fluctuations in the first seasons which includes a spike up to around 52% correctness in seasons 75~77, followed by a sharp decrease back to around 48% in season 78, and generally staying around the 48%~49% range throughout the 5 most recent seasons. Therefore,  if an individual participated in the seasons 75 instead of 74, one could potentially argue (of course this is very unsound) that on average, the individual who participated in 75 would be more likely to answer a question correct than someone who participated in season 74. Of course, there are lots of different confounding variables in this argument, where one could easily look at the distribution of individuals within the seasons and find that in season 75 there was an abundance of individuals who did very well (due to smarts, or even cheating) which lead to this jump from the previous season, and thus does not represent any ordinary individual's ability to answer questions correctly. Nevertheless, because we see variation in the seasons, it would be good to consider this in further analysis. Now that we have checked a higher-level view of the seasons, I will now dive into the matches-level data to see trends or anything that pops out in the regular season on aggregate (without distinguishing subgroups).


```{r}
regular_df %>%
  group_by(season,match) %>%
  summarize(mean_correct = mean(correct)) %>%
  arrange(season,match) %>%
  ungroup() %>%
  mutate(idx = row_number()) %>%
  ggplot(aes(x=idx,y=mean_correct)) +
  geom_line(color="black") +
  labs(title="Mean correct rate of all individuals through the regular matches",
       x="Matches over time") +
  theme(plot.title=element_text(face="bold",size=(13)))
```
From the graph above, we do not see any systematic trend over time, where these mean correct rates seems as though they were random draws from a normal distribution situated at round 50% (basically random noise), which is a very intuitive result. This indicates that (on average) there shouldn't be an underlying structural influence on the data due to different matches (i.e. it is not evident that there are some matches that are inherently rigged so that the mean correct rate is very high or low relative to every other match). This is already very granular data, and because we cannot see some matches being inherently different from other matches when only looking at the entire sample, there is no strong justification to be made about using this variable to make an argument for indicating individual's ability to score on well on a particular question.

Of course, a critic can easily say that the different years will have different trends and this overall trend is misleading. To that I offer the following graphs, which separate each year's matches by the respective years.

```{r}
by_match_t <- regular_df %>%
  group_by(season,match,year) %>%
  summarize(mean_correct = mean(correct)) %>%
  arrange(season,match,year) %>%
  ungroup() %>%
  group_by(year) %>%
  mutate(idx = row_number())

ts1 <- by_match_t %>%
  filter(year==2017) %>%
  ggplot(aes(x=idx,y=mean_correct)) +
  geom_line(color="black") +
  labs(title="Mean correct rates in 2017",
       x="Matches over time") +
  theme(plot.title=element_text(face="bold",size=(11)))

ts2 <- by_match_t %>%
  filter(year==2018) %>%
  ggplot(aes(x=idx,y=mean_correct)) +
  geom_line(color="black") +
  labs(title="Mean correct rates in 2018",
       x="Matches over time") +
  theme(plot.title=element_text(face="bold",size=(11)))

ts3 <- by_match_t %>%
  filter(year==2019) %>%
  ggplot(aes(x=idx,y=mean_correct)) +
  geom_line(color="black") +
  labs(title="Mean correct rates in 2019",
       x="Matches over time") +
  theme(plot.title=element_text(face="bold",size=(11)))

ts4 <- by_match_t %>%
  filter(year==2020) %>%
  ggplot(aes(x=idx,y=mean_correct)) +
  geom_line(color="black") +
  labs(title="Mean correct rates in 2020",
       x="Matches over time") +
  theme(plot.title=element_text(face="bold",size=(11)))

ggarrange(ts1,ts2,ts3,ts4,
          lables=c(2017,2018,2019,2020),
          ncol=2,nrow=2)
```
Here, even in the separate visual cases, it would be extremely hard to argue that there is some seasonality or similarly any inherent structural issues which will provide arguments to support including matches as an explanatory variable. 

Now that we have seen that, on average, the regular season data is basically random noise, there is no reason to go even more granular and look at questions data. This is because the matches data, which is already quite granular compared to season data, already displays quasi-random noise behavior. Because of this characteristic, even if we take matches data as an explanatory variable, it would already be approaching overfitting. As a result, the questions data, which are the components of the matches data, should not be introduced as an explanatory variable due to the extreme overfitting that it would bring into any model looking at the sample in general.

Lastly because the regular data set also provided some category data, it would be interesting to see if the categories are all centered at a particular mean with similar variances throughout the years, or that they are clustered differently.

```{r}
regular_df %>%
  group_by(category,year) %>%
  summarize(mean_correct = mean(correct)) %>%
  ggplot(aes(x=as.factor(category),y=mean_correct)) +
  geom_boxplot(fill="lightblue",color="steelblue") +
  labs(title="Distribution of mean correct rate of everyone throughout all regular season") +
  theme(axis.text.x = element_text(angle=60,hjust=1),
        plot.title = element_text(size=12)) +
  guides(fill=guide_legend(nrow=3))

regular_df %>%
  group_by(year,category) %>%
  summarize(perc = n()) %>%
  mutate(perc = perc/sum(perc)) %>%
  ggplot(aes(x=as.factor(category),y=perc,color=as.factor(year))) +
  geom_point() +
  theme(axis.text.x = element_text(angle=60,hjust=1)) +
  labs(title = "Percent of category appearance in regular season data") 
```
Notice that for each category, the distribution of the mean correct rate of everyone throughout the years have very different distributions. For example, throughout the years, people seem to do poorly relative to American history. This shows that each category has its own **"difficulty", where we define difficulty as an empirical discrepancy between the mean correct rate of particular categories, matches, or questions** and **NOT NECESSARILY** how intrinsically "hard" or "complicated" a question is.

Now that we have seen how each of our variables of interest is presented in the sample in general within the regular seasons. I will now turn to the champions data and look at variables within that dataframe accordingly.

Glimpsing into the champions data frame we find that the round date is somewhat parallel to the matches data in the regular season. Therefore, I will take a look at that first and see how it might differ from the regular season's matches data. This graph below will hope to demostrate the trends of the mean correct rate of final rounds over time, and see of there are any structural or inherent traits worth considering.

```{r}
by_round <- champions_df %>%
  group_by(year,round) %>%
  summarize(mean_correct = mean(correct)) %>%
  arrange(year,round) %>%
  ungroup() %>%
  mutate(idx = row_number())

by_round %>%
  ggplot(aes(x=idx,y=mean_correct)) +
  geom_line() +
  geom_point(shape=21,size=4,fill="#69b3a2") +
  labs(title="Mean correct rate of championship rounds over years 2017-2018",x="Rounds over time")
```
Notice that although there are not many data points (which warrants a slightly deeper dive into the question-level data), the mean correct rate of championship rounds do seem similar to a random draw from a normal distribution centered around 55%. This number is quite intuitive considering that the entire population in the regular season is centered around 50% (which includes these finalists), and this number is higher than the average regular season correct rate. Therefore, I tentatively claim that (at least looking very naively at the whole finalist sample), that the rounds played in the championships do not explain one's mean correct rate on average. However, due to the small amount of data, I will now dive into the question level analysis over time.

```{r}
champions_df %>%
  group_by(year,round,question) %>%
  summarize(mean_correct = mean(correct)) %>%
  arrange(year,round,question) %>%
  ungroup() %>%
  mutate(idx = row_number()) %>%
  ggplot(aes(x=idx,y=mean_correct)) +
  geom_line()+
  labs(x="Mean correct questions over time",
       title="Mean correct questions in the 2017-2018 championships")
```
Here, we see that the data seems very similar to that of the matches data in the regular season. On average, there seems to be no seasonality or underlying inherent properties which can explain one's ability to get a question correct. Like the matches date, if we split by the years, we have...

```{r}
by_question_finals_year <- champions_df %>%
  group_by(year,round,question) %>%
  summarize(mean_correct = mean(correct)) %>%
  arrange(year,round,question) %>%
  ungroup() %>%
  group_by(year) %>%
  mutate(idx = row_number())

chmp1 <- by_question_finals_year %>%
  filter(year==2018) %>%
  ggplot(aes(x=idx,y=mean_correct)) +
  geom_line() +
  labs(x="Mean correct questions over time",
       title="Mean correct questions in the 2018 championships")

chmp2 <- by_question_finals_year %>%
  filter(year==2019) %>%
  ggplot(aes(x=idx,y=mean_correct)) +
  geom_line() +
  labs(x="Mean correct questions over time",
       title="Mean correct questions in the 2019 championships")

ggarrange(chmp1, chmp2, lable=c(2018,2019),
          nrow=2,ncol=1)
```
Again, even spitting up the championships temporally for a more straight forward analysis, there does not seem to be (on aggregate) anything about specific questions to write home about regarding explaining one's ability to get a question correct. Therefore, I conclude via visual analysis that at least on average, there should be no explanatory power of questions on one's ability to get correct answers if we only look at the entire population answering the question.

Now that we have taken a look at the regular and championships data separately, I will now address the important issue (which is what the question actually asked) of looking at the difference between the finalists and the non-finalists. First, I will note that according to Prof. Levitt, the final questions are harder than the regular season questions. Therefore, all else equal, we should expect the point estimate of the mean correctness to be much lower in the championships relative to the regular season. Of course, intuitively, the people who are in the championships are selected from the regular season to be the best around, and judging by the sheer difference between the size of the two data frames, it must be that the finalists must be extremely outstanding in terms of correct rates to be selected in. To confirm these two ideas, I will look at a very high-level analysis of individuals who made it to the championships and see 1) if the championship participants (finalists) are indeed much better than those who are not qualified, and 2) if the questions in the championships are indeed a lot harder. Where 'hard' here is simply defined as the lack of a concentrated distribution of mean correct rate at the higher range (for example, concentrated around 70%~90%).

First to see if the championship participates are indeed much better than those who are not qualified for the finals.

```{r}
finalists_lst <- unique(champions_df$id)
finalist_2018 <- champions_df %>%
  filter(year==2018)
finalist_2019 <- champions_df %>%
  filter(year==2019)
finalist_overlap <- champions_df %>%
  select(id,year) %>%
  distinct() %>%
  group_by(id) %>%
  filter(n()>1)

finalists_lst_2018 <- unique(finalist_2018$id)
finalists_lst_2019 <- unique(finalist_2019$id)
finalists_lst_overlap <- unique(finalist_overlap$id)

regular_df %>% 
  mutate(is_finalist_2018 = ifelse(user_id %in% finalists_lst_2018,1,0),
         is_finalist_2019 = ifelse(user_id %in% finalists_lst_2019,1,0),
         is_finalist_ovrl= ifelse(user_id %in% finalists_lst_overlap,1,0)) %>% 
  group_by(is_finalist_2018) %>% 
  summarize(mean_corr = mean(correct),
            season = season) %>% 
  ggplot(aes(x = season, y=mean_corr)) +
  geom_line()
# 
# regular_df <- regular_df %>% 
#   mutate(is_finalist = ifelse(user_id %in% finalists_lst,1,0))
# 
# regular_ID_full <- regular_df %>% 
#   group_by(user_id,is_finalist) %>% 
#   summarize(mean_correct = mean(correct))
# 
# regular_ID_2019 <- regular_df %>% 
#   filter(year==2019) %>% 
#   group_by(user_id,is_finalist) %>% 
#   summarize(mean_correct = mean(correct))
# 
# regular_ID_2018 <- regular_df %>% 
#   filter(year==2018) %>% 
#   group_by(user_id,is_finalist) %>% 
#   summarize(mean_correct = mean(correct))
# 
# regular_ID_full %>% 
#   ggplot(aes(x=mean_correct,fill=as.factor(is_finalist))) +
#   geom_density(alpha=0.6)  +
#   labs(title="Finalist vs. non-finalists performance in regular season throughout")
# 
# reg_cohort_2019 <- regular_ID_2019 %>% 
#   ggplot(aes(x=mean_correct,fill=as.factor(is_finalist))) +
#   geom_density(alpha=0.6)  +
#   labs(title="Finalist vs. non-finalists performance in regular season 2019") +
#   scale_fill_discrete(name="",labels=c("Not Finalist","Finalist"))
# 
# reg_cohort_2018 <- regular_ID_2018 %>% 
#   ggplot(aes(x=mean_correct,fill=as.factor(is_finalist))) +
#   geom_density(alpha=0.6)  +
#   labs(title="Finalist vs. non-finalists performance in regular season 2018",
#        fill="") +
#   scale_fill_discrete(name="",labels=c("Not Finalist","Finalist"))
# 
# ggarrange(reg_cohort_2018, reg_cohort_2019, label=c(2018,2019),
#           nrow=2,ncol=1)
```
Here, we see a very intuitive outcome: Those who are in the finals have scored extremely well during the regular season. This shows that the intuition that the finalists are only those who are extremely outstanding in terms of correct percentage holds empirically.

Now looking into the second question, we expect that the finals should have more difficult (where we have already defined difficult as an empirically different mean correct rate, so here "more difficult" means "lower mean correct rate") questions relative to the regular season. This should be identified if we see that peak that is the finalists performance during the regular season flatten out and return to the lower ranges of the mean correct rates. The following graph presents the performance of finalists during the regular season and the championships.

```{r}
# filtered_ID <- regular_ID_full %>% 
#   filter(is_finalist==1) %>% 
#   select(-is_finalist) %>% 
#   rename(id=user_id)
# 
# joined_finalists <- champions_df %>% 
#   group_by(id) %>% 
#   summarize(mean_correct_final = mean(correct)) %>% 
#   left_join(filtered_ID,by="id") %>% 
#   gather(reg_final,correct_rate,mean_correct_final:mean_correct) %>% 
#   mutate(reg_final = case_when(reg_final=="mean_correct_final"~"final",
#                                reg_final=="mean_correct"~"regular"))
# 
# joined_finalists_2019 <- champions_df %>% 
#   group_by(id) %>% 
#   filter(year==2019) %>% 
#   summarize(mean_correct_final = mean(correct)) %>% 
#   left_join(filtered_ID,by="id") %>% 
#   gather(reg_final,correct_rate,mean_correct_final:mean_correct) %>% 
#   mutate(reg_final = case_when(reg_final=="mean_correct_final"~"final",
#                                reg_final=="mean_correct"~"regular"))
# 
# joined_finalists_2018 <- champions_df %>% 
#   group_by(id) %>% 
#   filter(year==2018) %>% 
#   summarize(mean_correct_final = mean(correct)) %>% 
#   left_join(filtered_ID,by="id") %>% 
#   gather(reg_final,correct_rate,mean_correct_final:mean_correct) %>% 
#   mutate(reg_final = case_when(reg_final=="mean_correct_final"~"final",
#                                reg_final=="mean_correct"~"regular"))
# 
# joined_finalists %>% 
#   ggplot(aes(x=correct_rate,fill=reg_final)) +
#   geom_density(alpha=0.6) +
#   labs(title="Finalist performace in regular and championships")
# 
# finalist_cohort_2018 <- joined_finalists_2018 %>% 
#   ggplot(aes(x=correct_rate,fill=reg_final)) +
#   geom_density(alpha=0.6) +
#   labs(title="Finalist performace in regular and championships 2018") +
#   scale_fill_discrete(name="",labels=c("During Regulars","During Champions"))
# 
# finalist_cohort_2019 <- joined_finalists_2019 %>% 
#   ggplot(aes(x=correct_rate,fill=reg_final)) +
#   geom_density(alpha=0.6) +
#   labs(title="Finalist performace in regular and championships 2019") +
#   scale_fill_discrete(name="",labels=c("During Regulars","During Champions"))
# 
# ggarrange(finalist_cohort_2018, finalist_cohort_2019, label=c(2018,2019),
#           nrow=2,ncol=1)
```
Notice that this graph is extremely similar to that above graph with different cohorts (finalists vs. non-finalists)! However, to be confused, this graph directly above shows the differential performance of the finalists during the regular season (outlined blue), and the championships(outlined red). This empirically shows our second idea that the championships are much harder in terms of correct rates is true. 

Furthermore, one could even argue that the finals questions have been designed so that it would exactly flatten the 'finalists' curve to have a similar distribution to that of the regular season participants. Let's see if this holds. Now that we have seen these two comparisons, lets check how the finalist do in the championship versus how non-finalists do in the regular rounds (basically overlaying the two red distributions from the previous two graphs)

```{r}
# sum_champs_2018 <- champions_df %>% 
#   group_by(id) %>% 
#   filter(year==2018) %>% 
#   summarize(mean_correct_final = mean(correct)) %>% 
#   rename(user_id=id)
# 
# sum_champs_2019 <- champions_df %>% 
#   group_by(id) %>% 
#   filter(year==2019) %>% 
#   summarize(mean_correct_final = mean(correct)) %>% 
#   rename(user_id=id)
# 
# sum_champs_full <- champions_df %>% 
#   group_by(id) %>% 
#   summarize(mean_correct_final = mean(correct)) %>% 
#   rename(user_id=id)
# 
# reg_fin_join_full <- regular_ID_full %>% 
#   left_join(sum_champs_full,by="user_id") %>% 
#   mutate(mean_correct = ifelse(is_finalist==1,mean_correct_final,mean_correct)) %>% 
#   select(-mean_correct_final)
# 
# reg_fin_join_2018 <- regular_ID_2018 %>% 
#   left_join(sum_champs_full,by="user_id") %>% 
#   mutate(mean_correct = ifelse(is_finalist==1,mean_correct_final,mean_correct)) %>% 
#   select(-mean_correct_final)
# 
# reg_fin_join_2019 <- regular_ID_2019 %>% 
#   left_join(sum_champs_full,by="user_id") %>% 
#   mutate(mean_correct = ifelse(is_finalist==1,mean_correct_final,mean_correct)) %>% 
#   select(-mean_correct_final)
# 
# reg_fin_join_full %>% 
#   ggplot(aes(x=mean_correct,fill=as.factor(is_finalist))) +
#   geom_density(alpha=0.6) +
#   labs(title="Regular season performance by non-finalists\n                           vs.\nChampionship performance by finalists",
#        fill="") +
#   scale_fill_discrete(labels=c("Regulars","Finalists"))
# 
# reg_fin_2018_gr <- reg_fin_join_2018 %>% 
#   ggplot(aes(x=mean_correct,fill=as.factor(is_finalist))) +
#   geom_density(alpha=0.6) +
#   labs(title="Regular season performance by non-finalists\n                           vs.\nChampionship performance by finalists\nin 2018",
#        fill="") +
#   scale_fill_discrete(labels=c("Regulars","Finalists"))
# 
# reg_fin_2019_gr <- reg_fin_join_2019 %>% 
#   ggplot(aes(x=mean_correct,fill=as.factor(is_finalist))) +
#   geom_density(alpha=0.6) +
#   labs(title="Regular season performance by non-finalists\n                           vs.\nChampionship performance by finalists\nin 2019",
#        fill="") +
#   scale_fill_discrete(labels=c("Regulars","Finalists"))
# 
# ggarrange(reg_fin_2018_gr,reg_fin_2019_gr)
```
```{r}

```

Indeed, it seems that the championships have 'flattened out' the mean correct rate of individuals who made the finals! Here, we see that on average, each group's distribution within their 'appropriate' difficulty of questions, as in, the finalists gets harder questions, and non-finalists gets normal questions, is relatively similar. However, we see a small bump near the 0.25 mark for the finalist group. While someone could argue that those who get very low scores indicate those who have "Cheated" in the regular season, and could thus lead to this skew, I do not believe this is the case. This is because unlike the regular season, "after day 2, the bottom performers are eliminated". Nevertheless, this discussion will be postponed to question 4, where we will dive deep into the question of cheating after we have constructed some models to predict individual correctness during the regular and finals season.

Finally, checking the categories between finalists and non-finalists, we find that...

```{r}
# regular_df %>% 
#   group_by(is_finalist,season,category) %>% 
#   summarize(mean_correct = mean(correct)) %>% 
#   arrange(category) %>% 
#   ggplot(aes(x=season,y=mean_correct,color=as.factor(is_finalist))) +
#   geom_line() +
#   facet_wrap(~category)
# 
# regular_df %>% 
#   group_by(is_finalist,season,category) %>% 
#   summarize(mean_correct = mean(correct)) %>% 
#   arrange(category) %>% 
#   mutate(is_finalist = case_when(is_finalist==0~"Regular",
#                                  is_finalist==1~"Finalist")) %>% 
#   spread(is_finalist,mean_correct) %>% 
#   mutate(diff = Finalist-Regular) %>% 
#   ggplot(aes(x=season,y=diff)) +
#   geom_line() +
#   facet_wrap(~category)
# 
# regular_df %>% 
#   group_by(is_finalist,season,match,year) %>% 
#   summarize(mean_correct = mean(correct)) %>% 
#   ungroup() %>% 
#   arrange(is_finalist) %>% 
#   group_by(is_finalist) %>% 
#   mutate(idx = row_number()) %>% 
#   ggplot(aes(x=idx,y=mean_correct,color=as.factor(is_finalist))) +
#   geom_line()
```
In this large collection of graphs, the mean correct rate of individuals who got into the finals vs. the mean correct rate of individuals who did not get into the finals is shown. Notice that for all the graphs, those who got into the finals will have a strictly higher level scores than those who didn't (which makes sense, because that's how they got into the finals in the first place). However, what is more interesting is the trends over time. In particular most of the categories (such as Television, Science, Classical Music, Current events etc.).

Furthermore, we see that each category is not likely drawn from an uniform distribution. This, combined with the fact that each category has on average different mean correct rates, means that this will be quite important to take into consideration when modeling and predicting whether or not an individual will get a random question right.

To sum up, I have shown that looking at simple aggregate data on seasons and matches in the regular season data, as well as round and questions data in the championship data, all exhibit random behavior which does not help to predict the correctness of an individual's answer. Furthermore, I have shown that for different categories, the mean correct rate throughout the years are clustered differently. Furthermore, we see that finalists are strictly better than non finalists in each category in terms of mean correct rates. This might lead to an predictive variable we can use to predict an individual's ability to get a particular question right. As seen in the data this holds because, if the individual is a finalist instead of a non-finalists, and the question is about American history and not art, then it must be that we should expect her to do better than the case where she was a non-finalist doing art (check figures on categories for concrete justification). Finally, I have shown that finalists and non-finalists have very different mean correct rates. In particular, finalists are far better at answering regular questions than non-finalists (since that's how they are chosen as finalists anyways). However, finalists performance drop dramatically once the actual championship questions are presented, where they share a very similar distribution in mean correct rates as the non-finalists in the regular season.

### Question 2:

Build a good model for predicting whether or a not a person will get a particular question right in the regular season. Describe why you made the modeling choices you made.

### Response:

For this question, the story is: if this is all the data that we have and we also have information about a specific question X in the regular season, then imagine a random individual A comes up to us and asks "given the data of the season, guess how I did when answering this specific question X?". i.e. this is a in-sample prediction using regular season data to predict regular season data.

At the core of this prediction, it boils down to two intuitive points that we need to ask ourselves before we respond to this individual A: how good at answering questions is individual A for answering a specific question type X? Motivated by these two points and connecting this with what we have discovered in question one, I believe a good model would include the following considerations:

To measure how "good" an individual is, I believe it is most intuitive to take do the following. Before anyone does the examination, each contestant is a blank slate with no information. Taking the questions, I will assume that this is a complete reflection of an individual's skill (notice that I am ruling out the existence of cheaters here). Therefore, I retrieve the information "how good are you at answering a specific question?". Based on the observed information, I am saying that "If the category is not hard for you (i.e. you got good scores in the past), then you shouldn't have a problem answering questions of the similar categories in the future, and vice versa". This will be the guiding principle of constructing my model.

In constructing the model, I will take this as the scenario: it is the case that we have both predictor and outcomes in the training sample (i.e., we have all the data before 2020), this is a in-sample prediction question, where we allocate regular season data in 2020 (id, question category, match number etc.) for testing purposes.

The model will make the assumptions/observations that:
  + individuals scores fully reflect their inherent abilities (no cheating)
  + question type is not uniform in difficulty, some questions are more difficult than others (categorical differences)
  + match, season, and year variation are uniform (no fixed effects)
  
Note that points 2 and 3 are proven empirically in question 1, where we have seen that different categories have different distributions, and that match, season, and year variables are like draws from a normal distribution, not worthy of fixed effects. The main lever that I will pull will be the individual's performance on specific categories of questions. i.e. if you were really good at economics/business before, I would have more faith on you to get the next economics/business question right than someone who really sucked at economics/business before.

<!-- Note that I have chosen year-level data because I want to capture a holistic picture one's recent ability to answer specific questions while not getting caught in the variation that occurs on the matches level. *Furthermore, since the match-level data is stationary for each subgroup, this will not necessarily be problematic in anyway since we do not have moving averages even if we look at the entire data set, so needless to say that match-level data for any given season is stationary as well. Therefore, the choice of using average scores on a season level will both capture the necessary information while not be biased or overfitting in any sense.* Furthermore, if we have individuals who have never attempted a question type, then their personal mean will be zero. This is not an issue because we are effectively stating "I have no idea how you will do in this question, so I cannot say that you will be better or worse than your peers" in our regression. Note that for the sake of easier interpretations of 'odds' in a logit regression, I will multiply the correct rate by 100. My justification for this is to turn the logit coefficients into ones interpretable by using the logic "for 1 unit increase (i.e. for 1% increase), how would I increase my odds in getting the question right". -->

Following that, we starting out with the extremely simple OLS model:

```{r}
library(aod)

q2_regression_df <- regular_df %>% 
  filter(year!=2020) %>% 
  #mutate(is_finalist = ifelse(user_id %in% finalists_lst_2018,1,0)) %>%
  group_by(user_id,category) %>% 
  mutate(id_cat_mean = mean(correct))

# test_lm <- glm(data=q2_regression_df, correct ~ (id_mean_correct_in_category) + (group_mean_correct_in_category),family="binomial")

test_lm2 <- lm(data=q2_regression_df, correct ~ id_cat_mean)

q2_test_df <- regular_df %>% 
  filter(year==2020) %>% 
  #mutate(is_finalist = ifelse(user_id %in% finalists_lst_2019,1,0)) %>%
  group_by(user_id,category) %>% 
  mutate(id_cat_mean = mean(correct))

predicted_possibilities_q2 <- predict(test_lm2, q2_test_df, type="response")

verdicts_q2 <- rep(0, dim(q2_test_df)[1])
verdicts_q2[predicted_possibilities_q2 > .5] = 1

test_results <- as.data.frame(cbind(verdicts_q2,q2_test_df$correct))

accuracy_q2 <- test_results %>% 
  rename(actual_results = V2,
         predicted_results = verdicts_q2) %>% 
  mutate(correct_prediction = ifelse(predicted_results==actual_results,1,0)) %>% 
  summarize(predicted_accuracy = mean(correct_prediction))

summary(test_lm2)
accuracy_q2
```
In this basic OLS regression, where we considered individual's performance as well as how difficult is this category among their peers. To restate, I have trained the model on the 2017~2019 subset and tested on the rest of the regular data of 2020. Although this extremely parsimonious prediction does give a fairly decent predicted accuracy of around 68.7%, we should still explain the model and the thought process involved.

In choosing this model, I have thought about some other variables 
  + score of individual with similar 'skill' levels (finalist vs. non-finalist)
  + seasonal/match/question/year fixed effects

However, I have chosen to omit all of them for a few reasons: 
  + relative difficulty simply does not sound like a good prediction for a *particular* person going for a *particular* question. Although the argument can be made that if we dont know anything about an individual, then surely relative difficulty of that question type for her cohort will give us some indication of her skill. However, it is apparent that this reasoning is flawed: *1)* how would someone who haven't answered questions be placed in a cohort? Thus the cohort argument requires individual's 'skill' (i.e. mean correct rates given a category), but then this information is already captured by the individual's performance of a given question type. *2)* Just because my peers are doing well, does that mean I also do well? Sure an argument can be made for a classroom setting, but because this is a web-based trivia competition, I cannot reasonably conclude that this is the case.
  + Following from our model assumptions of no fixed effects, I would further clarify that having fixed effects of this kind assumes that there is some inherent, structural difference between season to season, question to question. However, since we have shown that these pieces of data are basically random draws from a normal distribution (see question 1), adding these would be introducing unnecessary noise in our model.

To conclude, we have built a extremely parsimonious model where, by checking each individual's performance in each category type, we estimate how one's previous performance in the specific question type will influence the rate at which the same individual will get a question of the same category correct. Once we have completed the model, we get a point estimate of 1, which means that for every percent that an individual shows that they are good at a particular question type, we expect that they have a 1% more likely to get the question right next time. (**This is extremely simple and an extremely trivial result. However, this is to be expected given the bare-bones data that we have.** Say if we had education level data, then I expect this estimate would be different) In the end, I have checked to see that our model has an correct rate of 68.7% by training on the 2017~2019 data and testing on the 2020 data.

### Question 3:

Build a good model for predicting whether or not a person will get a particular question right in the first two rounds of the championship using ONLY information from the regular season. Describe why you made the modeling choices you made.

### Response:

The wording of this question is not entirely clear as to who and what are we predicting. Therefore, I will interpret the question as follows: What is the overall likelihood that a someone gets a championship question X correct. 

There are few pieces of information that we must consider before going into this question:

  + the finals questions are empirically more difficult than the regular season data (see question 1)
  
  + championships data do not provide any question categories
  
  + only an extremely small subset of individuals actual participate in the finals (approximately 2.5% of all participants).
  
  + we are limited to only running regressions on the regular season data, but looking at the finals data for general information (graphs) is allowed.
  
The main point of interest is with the first piece of information provided above, where we have already seen that the finals questions are very difficult relative to the regular questions (since the cluster of finalists which were skewed to the 80th percentile was pushed back to a normal distribution centered at around the 50th percentile). Therefore, the main type of questions that we must consider is the "more difficult questions", and we can no longer use the category variable directly here.

Now that we are only left with even more bare-bones data (without), and also that we are stuck with the regular season data, then I think the most intuitive thing to do here is to look at how individuals do in the hardest questions in the regular season as a "proxy" for the questions in the championships. 

Following our definition of difficulty, I must first choose what would be the cutoff for a "difficult question". While this sounds quite arbitrary, I will do my best to avoid introducing biases into the analysis. In particular, I will use a methodology that is extremely similar to the one from the previous question, where I simply substitute the "subject category" into the "difficult category". 

The model will make the assumptions/observations that:
  + individuals scores fully reflect their inherent abilities (no cheating)
  + only a small and selected subset of individuals actually participate in the finals (selectivity)
  + difficulty in the questions raise dramatically during the finals (difficulty)
  + match, season, and year variation are uniform (no fixed effects)
  
Once again, please note that points 2 and 3 are proven empirically in question 1: 1) a quick calculations looking at the number of IDs in the regular season vs the championships reveal that, throughout the years, there has only been 2.5% of all participants who were ever involved in the finals. 2) throughout time, the distribution for the finalists category moves from a sharp peak centered at around 80% correct rate in the regular season to a distribution highly resembling the distribution of mean correct rates for non-finalists during the regular season. 

Then, regression itself will take the highest 2.5% quantile (since only 2.5% of individuals are admitted into the finals), and check their performance on the most difficult question (so that the training data tries it's best to approximate the mean and standard deviation of the performance of finalists during the championships). The training will be conducted on the regular season data from 2017~2019, and the model will be tested on data from 2020.

**Please note: I am at no point constructing a model which is trained on championship data.**

```{r}
q3_df <- regular_df %>% 
  filter(year!=2020) %>% 
  group_by(user_id) %>% 
  mutate(overall_mean_correct = mean(correct)) %>% 
  group_by(season,match,question) %>% 
  mutate(mean_correct_of_q = mean(correct))

qtl <- quantile(q3_df$mean_correct_of_q,seq(0.01,1,by=(1/9)))
qtl_usr <- quantile(q3_df$overall_mean_correct,seq(0.98,1,by=(1/2)))

q3_df <- q3_df %>% 
  mutate(question_category=case_when(mean_correct_of_q <= qtl[1] ~ 1,
                                     mean_correct_of_q > qtl[1] & mean_correct_of_q <= qtl[2] ~ 2,
                                     mean_correct_of_q > qtl[2] & mean_correct_of_q <= qtl[3] ~ 3,
                                     mean_correct_of_q > qtl[3] & mean_correct_of_q <= qtl[4] ~ 4,
                                     mean_correct_of_q > qtl[4] & mean_correct_of_q <= qtl[5] ~ 5,
                                     mean_correct_of_q > qtl[5] & mean_correct_of_q <= qtl[6] ~ 6,
                                     mean_correct_of_q > qtl[6] & mean_correct_of_q <= qtl[7] ~ 7,
                                     mean_correct_of_q > qtl[7] & mean_correct_of_q <= qtl[8] ~ 8,
                                     mean_correct_of_q > qtl[8] ~ 9)) 
q3_df <- q3_df %>% 
  filter(question_category<4) %>% 
  filter(overall_mean_correct>=qtl_usr[1]) %>% 
  group_by(user_id) %>% 
  mutate(mean_correct = mean(correct)) %>%
  select(user_id,correct,mean_correct)

#notice this distribution is similar to the distribution of finals!
#running the regression
# 
# finals_lm <- lm(data=q3_df, correct ~ mean_correct)
# 
# summary(finals_lm)
# 
# q3_test <- champions_df %>% 
#   #filter(year==2018) %>% 
#   group_by(round,question) %>% 
#   mutate(mean_correct_of_q = mean(correct))
# 
# qtl_test <- quantile(q3_test$mean_correct_of_q,seq(0.01,1,by=(1/9)))
# 
# q3_test <- q3_test %>% 
#   mutate(question_category=case_when(mean_correct_of_q <= qtl_test[1] ~ 1,
#                                      mean_correct_of_q > qtl_test[1] & mean_correct_of_q <= qtl_test[2] ~ 2,
#                                      mean_correct_of_q > qtl_test[2] & mean_correct_of_q <= qtl_test[3] ~ 3,
#                                      mean_correct_of_q > qtl_test[3] & mean_correct_of_q <= qtl_test[4] ~ 4,
#                                      mean_correct_of_q > qtl_test[4] & mean_correct_of_q <= qtl_test[5] ~ 5,
#                                      mean_correct_of_q > qtl_test[5] & mean_correct_of_q <= qtl_test[6] ~ 6,
#                                      mean_correct_of_q > qtl_test[6] & mean_correct_of_q <= qtl_test[7] ~ 7,
#                                      mean_correct_of_q > qtl_test[7] & mean_correct_of_q <= qtl_test[8] ~ 8,
#                                      mean_correct_of_q > qtl_test[8] ~ 9))
# 
# q3_test <- q3_test %>% 
#   filter(round<3) %>%
#   group_by(id) %>% 
#   mutate(mean_correct = mean(correct))

## THIS IS THE INSAMPLE TEST

q3_test <- regular_df %>% 
  filter(year==2020) %>% 
  group_by(user_id) %>% 
  mutate(overall_mean_correct = mean(correct)) %>% 
  group_by(season,match,question) %>% 
  mutate(mean_correct_of_q = mean(correct))

qtl_test <- quantile(q3_test$mean_correct_of_q,seq(0.01,1,by=(1/9)))
qtl_usr_test <- quantile(q3_test$overall_mean_correct,seq(0.975,1,by=(1/2)))

q3_test <- q3_test %>% 
  mutate(question_category=case_when(mean_correct_of_q <= qtl_test[1] ~ 1,
                                     mean_correct_of_q > qtl_test[1] & mean_correct_of_q <= qtl_test[2] ~ 2,
                                     mean_correct_of_q > qtl_test[2] & mean_correct_of_q <= qtl_test[3] ~ 3,
                                     mean_correct_of_q > qtl_test[3] & mean_correct_of_q <= qtl_test[4] ~ 4,
                                     mean_correct_of_q > qtl_test[4] & mean_correct_of_q <= qtl_test[5] ~ 5,
                                     mean_correct_of_q > qtl_test[5] & mean_correct_of_q <= qtl_test[6] ~ 6,
                                     mean_correct_of_q > qtl_test[6] & mean_correct_of_q <= qtl_test[7] ~ 7,
                                     mean_correct_of_q > qtl_test[7] & mean_correct_of_q <= qtl_test[8] ~ 8,
                                     mean_correct_of_q > qtl_test[8] ~ 9))
q3_test <- q3_test %>% 
  filter(question_category<4) %>% 
  filter(overall_mean_correct>=qtl_usr[1]) %>% 
  group_by(user_id) %>% 
  mutate(mean_correct = mean(correct)) %>%
  select(user_id,correct,mean_correct)

##

predicted_possibilities_q3 <- predict(finals_lm, q3_test, type="response")

verdicts_q3 <- rep(0, dim(q3_test)[1])
verdicts_q3[predicted_possibilities_q3 > .5] = 1

test_results_q3 <- as.data.frame(cbind(verdicts_q3,q3_test$correct))

accuracy_q3 <- test_results_q3 %>% 
  rename(actual_results = V2,
         predicted_results = verdicts_q3) %>% 
  mutate(correct_prediction = ifelse(predicted_results==actual_results,1,0)) %>% 
  summarize(predicted_accuracy = mean(correct_prediction))

summary(finals_lm)
```
To conclude, we have created the most skilled individuals (top 2.5%) and considered their performance on the hardest questions (which approximates finals data the best to our ability, see below for graph). As a result, we have produced a OLS regression which states that, for every percent that a potential finalist is better at answering hard questions in the regular rounds, they are 1% more likely to actually get a particular question during the finals correctly. This is another unexciting result, but it seems to have a 64.8% prediction accuracy for our "mock-championship" represented by the hardest questions in 2020. 

NOTE: Below is the distribution of the regular season questions' mean correct rate among finalists used to train the model.

```{r}
q3_df %>% 
  group_by(user_id) %>% 
  ggplot(aes(x=mean_correct)) +
  geom_histogram() +
  labs(title = "Distribution of performance on the training questions used")

q3_test %>% 
  group_by(user_id) %>% 
  ggplot(aes(x=mean_correct)) +
  geom_histogram() +
  labs(title = "Distribution of performance on the testing questions used")
```

### Question 4:

Using your model from question 3, do you find compelling evidence that some players were cheating in the regular season?  What share of the players who qualified for the championship do you think cheated substantially in the regular season?  Whatâ€™s your evidence?

### Response

For the following question, I will define "Consistent Potential Cheaters" as those who have extremely high regular season scores (such as top 95 percentile), yet fall short in the finals (such as the bottom 5 percentile). 

Another potential type of cheater where they may intentionally leave some points off, and thus balancing out their scores to seem like they are a "normal contestant". However, this could be a great point of analysis. if we assume that the question categories are given at random *TAKE A LOOK AT IT*, then it should be the case that these individuals would encounter different categories of questions randomly. However, we could potentially find individuals who are "overly balanced", where they seems to know roughly the same across every single category with minimal exceptions (i.e. a Econ buff that somehow has a 70% mean correctness in every single other category and with very small variance between each category) despite the mean correct rates within categories themselves (see question 1). 

Since the question categories are completely random, and we have empirically seen that mean correct rates of different categories are different in distribution, and assuming that most people are **NOT** cheaters, then we might be able to detect potential cheaters in the data. In essence, it is simply leveraging whatever data that we have, that they might not have. 

Of course, more "Clever Cheaters" would be much more dynamic, but this requires far more resources than what I am able to muster within a week.

Since we are **NOT** restricted to **ONLY** using the model from question 3, I will then build upon what I have already built in question 3 and combine the aforementioned "Over Balanced" idea to help further identify potential cheaters.

Like most of this assignment game plan here follows a two-pronged approach: using my model from question 3, I will tag individuals who have a massive drop in performance during the finals season. Then, I will check if there are suspicious mean correct rates category-wise in the regular season for these individuals. If we see that these high performers in the regular season are "overly balanced", then we might have a potential cheater on our hands. 

First to sniff out the first type of "Consistent Potential Cheaters". I will take a look at my model which took a look at 2018 data to see if there could be individuals who fit the criteria of a "Consistent Potential Cheater".

```{r}
q4_analysis <- as.data.frame(cbind(q3_test_filtered,verdicts_q3)) %>% 
  rename(predicted_result=...9)

q4_analysis_summarized <- q4_analysis %>% 
  mutate(correct_prediction = ifelse(predicted_result==correct,1,0)) %>% 
  relocate(predicted_result,.after=correct) %>% 
  mutate(accuracy = ifelse(predicted_result==correct,1,0)) %>% 
  group_by(id) %>% 
  summarize(id_predictions = mean(accuracy)) %>% 
  arrange(id_predictions)

q4_analysis_summarized %>% 
  ggplot(aes(x=id_predictions)) +
  geom_histogram()
```
Just by looking at the individual predictions, we do have some individuals where we have failed to estimate their performance by quite a margin, where some missed the mark at less than 60%. Furthermore, it was calculated that the standard deviation of this distribution of accuracy for individuals was 0.0892, which means that there are few individuals who are around 2 standard deviations below the average. I will take these ids and check if they have any interesting charactoristics in the regular data.

```{r}
suspected_lst = c(10534,12043,2611,3657,3810,6900,11455,12166,12200)

q4_regular_sus <- q3_df %>% 
  filter(user_id %in% suspected_lst) %>% 
  group_by(user_id,category) %>% 
  mutate(mean_corr_of_cat_id = mean(correct)) %>% 
  relocate(mean_corr_of_cat_id,.after=category) %>% 
  arrange(user_id) %>% 
  ungroup() %>%
  group_by(category) %>%
  mutate(mean_correct_of_cat_gen = mean(correct)) %>% 
  select(user_id,category,mean_corr_of_cat_id,mean_correct_of_cat_gen) %>% 
  distinct()
  
q4_regular_sus %>% 
  gather(self_or_group,mean_score,mean_corr_of_cat_id:mean_correct_of_cat_gen) %>% 
  mutate(self_or_group = case_when(self_or_group=="mean_corr_of_cat_id"~"self",
                                   self_or_group=="mean_correct_of_cat_gen"~"group")) %>% 
  ggplot(aes(x=as.factor(category),y=mean_score,color=as.factor(self_or_group))) +
  geom_point() +
  facet_wrap(~user_id)
```

Among all the individuals, there is one who is very interesting. **ID number 12043 fulfills all the requirements for a potential cheater**, where not only did the prediction vastly mispredicted her abilities, but also that her correct rate among all categories seems up to par, if not very high. Therefore, it follows from my definitions that I have identified one "potential cheater" among the entire finalist sample by using the model constructed in question 3.

However, I would argue that there is a better way of detecting cheaters, using the visualizations below, we might seek a new way:

*CONTINUE HERE*

As mentioned by the data description, even within the finalists, there are two groups that we need to consider: those who were eliminated halfway through (i.e. could not participate past the second round), and those who were not. Now, I will split the Finalists group into the two aforementioned components. In doing so, we can see exactly how different are the two groups. This will serve as a starting point for the question on whether or not there are cheaters. (4)

```{r}
past_filter <- champions_df %>% 
  group_by(id) %>% 
  filter(round>2)

past_filter_list = unique(past_filter$id)

champions_df %>% 
  mutate(to_end = ifelse(id %in% past_filter_list,1,0)) %>% 
  group_by(id,to_end) %>% 
  filter(round<=2,year==2019) %>% 
  summarize(mean_correct = mean(correct)) %>% 
  ggplot(aes(x=mean_correct,fill=as.factor(to_end))) +
  geom_density(alpha=0.6) +
  labs(title="Finalist performance in first two rounds by elimination 2019")

champions_df %>% 
  mutate(to_end = ifelse(id %in% past_filter_list,1,0)) %>% 
  group_by(id,to_end) %>% 
  filter(round<=2,year==2018) %>% 
  summarize(mean_correct = mean(correct)) %>% 
  ggplot(aes(x=mean_correct,fill=as.factor(to_end))) +
  geom_density(alpha=0.6) +
  labs(title="Finalist performance in first two rounds by elimination 2018")
```
Here, we would have expected some difference between the two distributions for both 2018 and 2019 data; since those who are eliminated are precisely those who do worse, this graph makes sense. Looking at each graph individually, the 2018 graph shows that those eliminated were extremely worse off relative to those who proceeded. Also worth noting is that those who move forward holds a bimodal distribution, meaning that they could possibly be indicating two subgroups, those who barely got through, and those who are truly good at trivia (since cheating is impossible).

However, there is a very interesting component to the 2019 graph: those who were eliminated show a somewhat bimodal distribution as well. While we have already explained that there might be a different subgroups in those who move onwards, where there could be trivia nerds who are simply amazing at trivia and those who are still knowledgeable enough to make it, but the questions are harder in the finals. This highlights why the individuals who did extremely well in the regular season might have slipped down in correctness rates a bit. The final rates still seem reasonable, where they could get the most (around 60%) of the questions correct. However, the same reasoning can be applied to the eliminated group: the red peak which is most right can be interpreted as simply those who entered the finals, but the individual's ability was just not enough. The most interesting aspect of the graph above is the "hump-shaped" distribution at around 20%~30%. In particular, how is it that they are slipping so low relative to their peers who are facing the same questions? If this group was indeed "homogenous" in the sense that they are all really good trivia buffs but some just happen to do poorly by chance, why would there be a large concentration of individuals who scored so low? I will do a quick histogram to find if there is indeed a separation of two groups within the eliminated group

```{r}
champions_df %>% 
  mutate(to_end = ifelse(id %in% past_filter_list,1,0)) %>% 
  group_by(id,to_end) %>% 
  filter(round<=2,year==2019,to_end==0) %>% 
  summarize(mean_correct = mean(correct)) %>% 
  ggplot(aes(x=mean_correct)) +
  geom_histogram(alpha=0.6,bins=12) +
  labs(title="Finalist performance in first two rounds by elimination 2019")

champions_df %>% 
  mutate(to_end = ifelse(id %in% past_filter_list,1,0)) %>% 
  group_by(id,to_end) %>% 
  filter(round<=2,year==2018,to_end==0) %>% 
  summarize(mean_correct = mean(correct)) %>% 
  ggplot(aes(x=mean_correct)) +
  geom_histogram(alpha=0.6,bins=12) +
  labs(title="Finalist performance in first two rounds by elimination 2019")
```


This raises the question about whether or not the group which entered the finals is indeed a "homogonous" group in the sense that their skills are indeed fully represented by their correct rates in the regular season Since a normal distribution will indicates that there is no inherent or structural difference between the groups (and distribution is simply variation), this bimodal distribution is curious because it hints at that there might be something more inherent happening. Furthermore, there is also "hump"

```{r}
champ_year <- champions_df %>% 
  group_by(year) %>% 
  summarize(mean_correct=mean(correct)) %>% 
  mutate(is_reg = 0)

reg_year <- cat_df %>% 
  filter(year %in% (2018:2019)) %>% 
  group_by(year) %>% 
  summarize(mean_correct=mean(mean_correct)) %>% 
  mutate(is_reg = 1)

joined_1819 <- full_join(champ_year,reg_year)

joined_1819 %>% 
  ggplot(aes(x=year,y=mean_correct,color=as.factor(is_reg))) +
  geom_point()

reg_cat_year <- cat_df %>% 
  filter(year %in% (2018:2019))

joined_full_1819 <- full_join(champ_year,reg_cat_year) %>% 
  select(-is_reg) %>%
  mutate(category=replace_na(category,"FINAL"))

joined_full_1819 %>% 
  ggplot(aes(x=year,y=mean_correct,color=as.factor(category))) +
  geom_point() +
  geom_point(data=champ_year,
             aes(x=year,y=mean_correct),
             color='red',
             size=3) +
  geom_point(data=reg_year,
             aes(x=year,y=mean_correct),
             color='blue',
             size=3)

```

This is because the finals data does not distinguish based on category. Therefore, if we do a difference in means analysis, (assuming we may run the risk of claiming people that do not get. However, there is something interesting: it seems that in 2018, the finals and regulars were quite closely spaced together, yet for some reason, there seems to be a large jump in correct rate in the 2019 finals. However, the 2019 case is sort of what we expect! Since the championships select the best of the best, we definitely would expect some sort of difference in the mean_correct variable if the people at the finals were indeed better at taking questions on aggregate. This brings up the possibility: what if it is the case that people cheated more in 2018, and thus the correct rate of the "amateur" group in the regular season nearly matches the "pro" group who actually went to the finals? i.e. it is not necessarily that the finalists cheated (which should be interesting to check if finalists have relatively the same trend in regular and final situations), but it could the case that people **cheated more in 2018, but still could not make the finals**, and thus **inflating the correct rate?**

### Question 5:

Using whatever information we gave you, come up with your best prediction for the number of correct answers each player who qualified for the 2020 championship got correct in that championship. Describe your modeling choices. You will be graded based on the accuracy of your predictions.

### Response: 

In particular, for the finalists, I will assign different 'points' to finalists based on how difficult the question was (i.e. whether or not everyone else got a particular question wrong). These points will be given based on 10 bins of percentiles, i.e. getting one of the most difficult of the questions will net the individual 1 points, where as getting one of the most easy 20% of the questions will net the individual 1 point. Now to our analysis, I will train on the 2018 regular season data and test on the 2018 finals data.

```{r}

df %>% 
  lm(corrup ~ censor + year_dummy + city_dummy)

```

