---
title: "R Notebook"
output: html_notebook
---

## Question 1:

It seems that the data may need to be cleaned. Before doing this, though, estimate the treatment effect of attending LLS in two ways. First, use a simple difference in test dist means of those who won or lost the lottery. Second, include covariates as controls (think OLS) and re-estimate the treatment effect. Report your estimates and standard errors from each technique. Are the results plausible?

### Step 1: Importing and getting to know the data

First, I will load the required libraries and check some basic charactoristics of the data. Specifically, checking the data 
type (chr,dbl,... etc), as well as noting the variables at hand
and number of observations (rows). Until otherwise stated, I will assume that those who entered the lab school were indeed enrolled through a 'valid lottery', meaning that it is a random sample of the population in question.


```{r}
library(tidyverse)
library(ggplot2)
library(stringr)
library(ggpubr)

raw_data <- read_csv("assignment1_data.csv")

print("Glimps of the dataframe 'raw_data'")
glimpse(raw_data)
print("Descriptive Statistics for 'raw_data'")
summary(raw_data)
```

Note that there are multiple items that jumps out, (take the -99 min income for example). While we should usually take a more detailed look at the data as well as clean the data *before* doing any regressions, we will follow the direction of the question and reserve these processes to the next question (Question 2).

### Step 2: Estimating treatment of attending lab school

#### Step 2.1: Estimating treatment without covariates

Now to address the question: estimating the treatment effect of attending lab school (variable: *lab*) on current scores (variable: *score*). According to the question, the following directly measures the treatment effect on the raw data without considering covariates. I will give a summary output directly following each model and at the end give a verbal discussion on both models. 

```{r}
no_covariate_scores <- lm(data = raw_data, score ~ lab)

summary(no_covariate_scores)
```
#### Step 2.2: Adding covariates as controls

In adding more covariates, I believe that past performance on tests will give information on the current performance on tests. Therefore, I will be adding the past scores data (variable: *past_score*) into the model as a covariate to control over.

```{r}
covariate_scores <- lm(data = raw_data, score ~ lab + past_score)

summary(covariate_scores)
```


Looking at the summary of the models above along with the descriptive statistics of the *lab* and *score* variables, it is extremely difficult to come up with a good explanation as to what is going on. Not only is the coefficient on the *lab* variable statistically insignificant (p-value at 0.355 for the first model, and 0.110 for the second), but when taking the summary statistics of *lab* and *score*, it also lacks a clear economic interpretation and significance. 

In particular, notice the scores range from 1.88 to 1312.939 yet the model reports the coefficient on the *lab* regressor to be 2.507 for the first model, and 4.363 for the second. This is a very insignificant amount when looking at the large range of test scores and does not lead to any sound economic interpretations as to the efficacy of attending the lab school (not to mention that the *lab* variable itself has a max value of *3*, where if it were only a dummy indicating whether or not an individual is attending lab school, we would expect *{0,1}*). Due to all these complications, the results here lack any statistical and economic significance to draw any plausible conclusion.

### Question 2:

Clean the data. This may or may not require you to delete, transform, standardize, and scale observations or columns. Whatever you do, describe what you did with specificity and why you did it.

### Step 1: A deeper dive into the raw data:
#### Step 1.1: Looking at continuous variables

First, I will generate graphs for each of the variables to get a high-level understanding of the data we are dealing with. Here, I will take a look at scores, past scores, incomes, and distances from the lab school. For each of the four graphs, it will be a simple histogram to show the frequency of each observation. The x-axis will be the actual values found in the data, and the y-axis will be the count of particular observations. Furthermore, I will show the summary statistics of each variable after the graphs, as well as checking for NA values in each of the variables with graphs

```{r}
working_df <- raw_data  #make a working dataframe and leave the raw data intact

gp_income <- working_df %>% 
  ggplot(aes(x=income)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Incomes") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_scores <- working_df %>% 
  ggplot(aes(x=score)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Scores") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_pscores <- working_df %>% 
  ggplot(aes(x=past_score)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Past Scores") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_dist <- working_df %>% 
  ggplot(aes(x=dist)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Distances from Lab") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_long <- working_df %>% 
  ggplot(aes(x=long)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Longitude") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_lat <- working_df %>% 
  ggplot(aes(x=lat)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Latitude") +
  theme(plot.title = element_text(size=11,hjust=0.5))

ggarrange(gp_pscores,gp_scores,gp_income,gp_dist,
          ncol=2,nrow=2)
ggarrange(gp_long,gp_lat,
          ncol=2,nrow=1)

#Note, two groups of graphs will be generated but not displayed in one screen, please navigate between the two!
```

```{r}
cat("Summary Statistics for Past Score\n")
summary(working_df$past_score)
cat(" Number of NA values:", sum(is.na(raw_data$past_score)), "\n\n")
cat("Summary Statistics for Current Score\n")
summary(working_df$score)
cat("  Number of NA values:", sum(is.na(raw_data$score)), "\n\n")
cat("Summary Statistics for Income\n")
summary(working_df$income)
cat("  Number of NA values:", sum(is.na(raw_data$income)), "\n\n")
cat("Summary Statistics for Distance\n")
summary(working_df$dist)
cat("  Number of NA values:", sum(is.na(raw_data$dist)), "\n\n")
cat("Summary Statistics for Longitude\n")
summary(working_df$long)
cat("  Number of NA values:", sum(is.na(raw_data$long)), "\n\n")
cat("Summary Statistics for Latitude\n")
summary(working_df$lat)
cat("  Number of NA values:", sum(is.na(raw_data$lat)), "\n\n")
```

I will summarize the information for each variable and potential next steps in data wrangling:

(First, rejoice that we didn't find any NA values.)

1. Past Scores:
  + I believe this shows a very plausible and convincing distribution. This is because, assuming standardized testing (such as the SAT) will often times normalize/curve their scores according to a distribution. Thus this gives us a very convincing collection of data because it fits a seemingly normal distribution.
  Furthermore, there are no extreme outliers, negative values, NAs, or anything out of the ordinary.

2. (Current) Scores:
  + Recall that I did not restrict the width of the histogram, and thus the histogram in question will include all instances of scores. This visual aid indicates that, while most entries are situated around 0, there are extreme outliers in this data (such as the max value of 1312.939) since the median is situated at 8.251. Given the cleaner nature of past scores, I will make the assumption that the scoring scheme for this standardized testing has not changed overtime (as there is still quite a lot of values situated near 0), and use *past scores as a reference to clean the current scores data.*
  
3. Income:
  + The graph for income shows a relatively normal distribution similar in shape to that of the past scores (which seems more or less clean). However, there are more than 900 values situated *below zero*. This requires further investigation because we see from the summary statistics that the minimum value is *-99*, which like the infamus *-999* will require careful cleaning.

4. Frequency in Distance from Lab:
  + Similar to the current scores, the visual representation of this graph shows a strong concentration of observations near zero, and outlying values which stretch to 13609.374. However, it could very well be a common error regarding units (i.e. some inputs kilometres while others inputs metres). To justify my thinking (assuming that the difference in units is just a two-way difference such as km and m), **I will see if scaling down the outlying values (starting from around 2500 units) will give us a more sensible result**.
  
5. Longitude and Latitude:
  + These two variables are talked about together, as I do not believe that one can truly make sense of these variables. The reason for my claim is that for both variables, the variable's 3rd quartile is 0, which I will interpret as no information gathered (rather than 7000+ people living somewhere in the Atlantic ocean). I will concede that for the non zero values, there could be some argument that can be made there. However, even in that case, I still believe that it will not be useful for our analysis on the treatment effect of lab school education. This is because the only reasonable and pertinent argument that can be made is the location between on the individual's home and the lab school may have some importance. However, this boils down to distance to the lab school, which is already covered by the *dist* variable. Therefore, I will disregard these two variables for now.

#### Step 1.1: Looking at categorical variables

Now I will repeat the visualization and summarization process as shown above, but this time for the categorical variables. Here, I will be using bar charts to visualize the variables gender, race, sports and lab school attendance. This is meant to see the unique values of each categorical variable and their frequency. In doing so, we may catch repeating values (such as 'male' and 'M') amongst other inconsistencies.
```{r}
gp_gender <- working_df %>% 
  ggplot(aes(x=gender)) +
  geom_bar(fill='blue',alpha=.7) +
  ggtitle("Frequency of Genders") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_race <- working_df %>% 
  ggplot(aes(x=race)) +
  geom_bar(fill='blue',alpha=.7) +
  ggtitle("Frequency of Race") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_lab <- working_df %>% 
  ggplot(aes(x=lab)) +
  geom_bar(fill='blue',alpha=.7) +
  ggtitle("Frequency of Attendance to Lab") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_sport <- working_df %>% 
  ggplot(aes(x=plays_sports)) +
  geom_bar(fill='blue',alpha=.7) +
  ggtitle("Frequency of Sports") +
  theme(plot.title = element_text(size=11,hjust=0.5),
        axis.text.x = element_text(angle=60,hjust=1))

gp_incbuck <- working_df %>% 
  ggplot(aes(x=income_bucket)) +
  geom_bar(fill='blue',alpha=.7) +
  ggtitle("Frequency of Income Buckets") +
  theme(plot.title = element_text(size=11,hjust=0.5))

ggarrange(gp_gender,gp_race,gp_lab,gp_incbuck,
         ncol=2,nrow=2)

gp_sport #showed separately due to squeezing names
```

```{r}
cat("->Counts for unique entries in gender")
table(working_df$gender)
cat("  Number of NA values:", sum(is.na(working_df$gender)), "\n\n")
cat("->Counts for unique entries in race")
table(working_df$race)
cat("  Number of NA values:", sum(is.na(working_df$race)), "\n\n")
cat("->Counts for unique entries in lab school attendance")
table(working_df$lab)
cat("  Number of NA values:", sum(is.na(working_df$lab)), "\n\n")
cat("->Counts for unique entries in sports")
table(working_df$plays_sports)
cat("  Number of NA values:", sum(is.na(working_df$plays_sports)), "\n\n")
cat("->Counts for unique entries in income buckets")
table(working_df$income_bucket)
cat("  Number of NA values:", sum(is.na(working_df$income_bucket)), "\n\n")
```
Like before, I will summarize the findings:

Again, rejoice in that we do not have NA values.

1. Gender, Race, and Sports:
  + I am grouping these three variables to talk about together because they share very similar issues. In particular, while they do not exhibit any NA values, these three variables have **different categories which are, in actuality, the same category**, for example, in the gender data, there is "M" and "male", where if I make a safe assumption that M really *means* male, then these two categories point to the exact same category, yet they are seperatly counted. Same situation occurs with "H" and "hispanic" for the race categorical variable. For the sports variable, we see first a set of ${0,1}$, where I interpret as {No Sports, Plays Sports}. However, we see then specific sports such as basketball, baseball etc. Therefore, I believe that these specific sports separated from the variable 1 can actually be combined into the value '1'. Therefore, the next step is to combine redundant categories in these variables
  
2. Attendance to Lab:
  + Assuming that this variable indicates whether or not the individual goes to the lab school, it is completely reasonable to expect a ${0,1}$ dummy variable. However, we see values such as "3" and "-1". At first glance, I could possibly make the (very shaky) argument that "-1" is an input error (typo) of "1", but "3" really cannot be explained with information at hand. Because attendance to the lab school is critical for our analysis, I believe that the best course of action is to drop these columns for the fear of making an incorrect assumption and skewing the results. 
  
3. Income Bucket:
  + While there are no explicit/glaring issues with this data, it is important to recall from the previous analysis on continuous variables that income itself is quite problematic (with -99s). If we make the assumption that these income buckets are directly constructed via the income column, then we are bound to have errors in analysis if we use this variable. In all, I believe that it would be ideal to clean the income variable first, then perhaps reconstruct our own income buckets by using the same bins if necessary.
  
#### Step 1.3: ID

I have reserved the ID variable to a section of its own because it deserves special attention. This is because identification should be representative of an unique observation (which means that each row, which represents a unit of observation, should have their own ID). Therefore, this will be the key to conduct all the aformentioned data cleaning. The only item to check is to see whether or not the IDs are unique. In the following function, I will return all the ID values that are unique.
```{r}
sum(duplicated(working_df$student_id) == TRUE)
```
It looks like that there are 9994 IDs that are duplicated. Therefore, the next course of action is threefold:
  + to delete completely identical rows
  + to delete rows with duplicated ids yet extremely faulty information in one specific category (i.e. for two ID entries, all else the same, one has a -99 in the income category; then delete the one with the -99 in the income category)
  + to carefully the rows with duplicated ids, but both has very 'believable' data.
  
  
### Step 2: Cleaning the data accoring to our observational conclusions

In the following sections, I will clean each of the indicated variables which require cleaning, provide visualizations and summaries along the way, and track my modifications

#### Step 2.1: Cleaning continuous data:

First, I will deal the -99s identified within income. I will follow the direction indicated in our class: I will first set the all the -99 to zero. However, recall that we have double counted IDs, and thus there is a possibility where we can reasonably delete the rows with faulty data (i.e. if all else equal, two identical rows only differ in income, where one has -99 and the other has a more believeable value, I believe it would be sensible to delete the row with the faulty income data). With that in mind, I will change -99s to 0s in case of a regression, and identify rows where we might delete the rows outright.

```{r}
sum(working_df$income < 0)
#1115

working_df <- working_df %>% 
  mutate(was_neg99 = ifelse(income == -99, 1,0)) %>% 
  mutate(income = replace(income, income < 0, 0)) #directly change all negatives to 0
  
```

Now we will look at scores. As previously noted, past scores seems to have a fairly reasonable distribution, meaning no NAs, extreme outliers, and follows a normal distribution as we expect from a standardized test. Judging by what I found previously with the past scores and current scores, I will use the past scores as a reference point to determine what the 'reasonable' score range should be for the current scores.
```{r}
summary(working_df$past_score)
summary(working_df$score)
```

Judging by the the summary stats and visualizatios, I believe that the maximum score on the standardized test, assuming that how the test calculates scores is consistent between the two times, the tests scores should be upwardly bounded around 15. First, I will do a very "hardcoded" operation to tease out the local upper boundaries of the group of scores near 0 on the visualization.

```{r}
for (val in c(13,14,15,20,50)){
  cat("this is the number of scores above ",val,":",sum(working_df$score > val),"\n")
}
```
Even in this very "hardcoded" or "theoryless" operation, we are able to tease out how many scores are above a certain cutoff. In doing so, we can more clearly see what the cutoff could be. Now, making the assumption for now that 15 is our cut off, I would like to see what is the **maximum** value that is **below** 15 and the **minimum** value that is **above** 15. In doing so, I wish to show that, under the assumption that scores should be roughly normal, we have no reason to expect a very large gap in scores
```{r}
below15 <- working_df %>% 
  select(score) %>% 
  filter(score<=15) 
above15 <- working_df %>% 
  select(score) %>% 
  filter(score>15)

below15 %>% 
  summarize(max = max(below15))
above15 %>% 
  summarize(min = min(above15))
```
Notice that there is a big jump from the cutoff below15 and above15! The max value below 15 is 14.302 where as the min value above 15 is 277.258. Furthermore, I will split the score data from 15 and check the distributions for those scores below and those scores above:
```{r}
gp_below15 <- below15 %>% 
  ggplot(aes(x=score)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Scores <15") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_above15 <- above15 %>% 
  ggplot(aes(x=score)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Scores >15") +
  theme(plot.title = element_text(size=11,hjust=0.5))

ggarrange(gp_below15,gp_above15,
         ncol=2,nrow=1)
```
Here, we get a very interesting result! Once we subdivide the score variable to those above 15 and below 15, we find that each looks somewhat normally distributed, which is exactly what we expect from a standardized exam score. Now looking at the score ranges of the two series, I suspect that the scores above 15 are created due to a missing decimal point which caused all data to be scaled up by 100 times. To show this, I will divide the scores from above 15 by 100 and plot it together with the scores below 15 on the same graph. The following histogram shows how a rescaled version of scores above 15 fits into the overall distribution. If we do not find any outliers here, I reasonably conclude that the rescaling was successful, and that my hypothesis about the resealing factor of 100 is a reasonable argument.
```{r}
rescaled_above15 <- above15/100
rescaled_score <- rbind(rescaled_above15, below15)

count(below15)
count(rescaled_above15)
count(rescaled_score)

rescaled_score %>% 
  ggplot(aes(score)) +
  geom_histogram(data = rescaled_above15, fill = "black", alpha = 0.8) + 
  geom_histogram(data = below15, fill = "purple", alpha = 0.4) +
  geom_histogram(data = rescaled_score, fill = "blue", alpha = 0.3)
```
Here, the black component of the histogram at the very bottom of the graph shows the rescaled counts above 15. The purple component of the histogram shows the original scores of scores below 15. The light blue component which sticks out at the top shows the result of the compounding of the two aformentioned subcomponents. Nevertheless, this shows that there are no significant outliers, and that the rescaling fits within the normal distirbution which we expect from a standardized test. Therefore, my conclusion is that I will move forward and clean the score data by dividing all scores > 15 by the factor of 100. (I also include a confirmation plot and summary stats on scores to indicate the successful transformation)
```{r}
working_df <- working_df %>% 
  mutate(score = ifelse(score>15, score/100, score))

working_df %>% 
  ggplot(aes(x=score)) +
  geom_histogram(fill="purple",alpha=0.2)
```

### Step 2.3: Distance

Now I will deal with distance the same way I have dealt with scores, which is checking for a scaling factor which we can use to bring the outliers back into the distirbution we are expecting. (Therefore I will no longer be explicitly going through all my thinking and instead provide short signposts) Here, we are not explicitly expecting a normal distribution for distance (and maybe at most even say that we expect a distirbution skewed to zero, since we expect people who goes to the school lives close to the school, but even that expectation is not completely reasonable, since they could very well just commute a longer distance).

First, recall the visualization of distance:
```{r}
gp_dist
```
This time, we dont have a reference, but judging by the visualization, there could be a cut at around 2000 units, therefore:
```{r}
for (val in c(2500,2250,2000,1500,1000,750,500,250,100,50,20,15,10)){
  cat("this is the number of distance above ",val,":",sum(working_df$dist > val),"\n")
}
```
Again, I would like to concede the brute-force nature of this analysis, but this nevertheless indicates at least a few cutoffs we can use, since we have found multiple 'stable' counts at different values. For the sake of argument, let us take 20 as the cutoff and find the max below 20 and the min above 20, and see if this is a valid cutoff.
```{r}
below20 <- working_df %>% 
  select(dist) %>% 
  filter(dist<=20) 
above20 <- working_df %>% 
  select(dist) %>% 
  filter(dist>20)

below20 %>% 
  summarize(max = max(below20))
above20 %>% 
  summarize(min = min(above20))
cat("the difference between max(below20) and min(above20) is:", abs(max(below20)-min(above20)))
```
As well as the graphs of the two:
```{r}
gp_below20 <- below20 %>% 
  ggplot(aes(x=dist)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Distances <20") +
  theme(plot.title = element_text(size=11,hjust=0.5))

gp_above20 <- above20 %>% 
  ggplot(aes(x=dist)) +
  geom_histogram(fill='blue',alpha=.7) +
  ggtitle("Frequency of Distances >20") +
  theme(plot.title = element_text(size=11,hjust=0.5))

ggarrange(gp_below20,gp_above20,
         ncol=2,nrow=1)
```
Notice that neither distributions have glaring issues such as extreme outliers. As there is no theoretical reason why I believe the distances should be normally distributed, it is more important that the two subsets of data are visually extremely similar. As previously, I suspect that there is a scaling issue and thus divide the distances > 20 by the factor. Here, judging by the indications on the x-axis, I will choose 1000 as the factor, as there is not only empirical evidance to do so (x-axis), there is also a theoretical reason where I suspect the different sets of data here is operating in kilometres and metres (1km = 1000m, just in case).
```{r}
rescaled_above20 <- above20/1000
rescaled_dist <- rbind(rescaled_above20, below20)

rescaled_dist %>% 
  ggplot(aes(dist)) +
  geom_histogram(data = rescaled_above20, fill = "black", alpha = 0.8) + 
  geom_histogram(data = below20, fill = "purple", alpha = 0.4) +
  geom_histogram(data = rescaled_dist, fill = "blue", alpha = 0.3)
```
Here, the dark blue component is the rescaled data from above 20 units. The middle shaded purple is the data from below 20 units. The light blue component is the aggregation of the two aformentioned components. Because the visual graph does not show extreme outliers or anything out of the ordinary, it seems that the rescaling was able to capture the outliers back into the distirbution that we expected. The theoretical explaination for this has been mentioned in passing, but to restate, I beleive that there was an entry error where individuals confused the units *km* and *m*, and thus explains the discrepency in the factor of 1000. (i.e. it could be that one researcher knows that the unit is *km*, while the other one thought they were using *m* instead)

Now cleaning the data:
```{r}
working_df <- working_df %>% 
  mutate(dist = ifelse(dist>20, dist/1000, dist))

working_df %>% 
  ggplot(aes(dist)) +
  geom_histogram(fill="blue", alpha=0.2)
```
Now moving to logitudal and latitudal data. Since I cannot reasonably find any use for the longitude and latitude data (*see full justification in Q2, section 1.1, summary point 5.*) I will proceed to delete these dataframes from our discussion to streamline the dataframe.
```{r}
working_df <- working_df %>% 
  select(-long,-lat)
```

#### Step 2.2 Cleaning categorecal data:

Recall the unique values in the different categorical variables race, gender, lab, and sports:

```{r}
working_df %>% 
  distinct(race)
working_df %>% 
  distinct(gender)
working_df %>% 
  distinct(lab)
working_df %>% 
  distinct(plays_sports)
#Note, each result shows in a different table
```
Notice that race has entries "hispanic" and "black" along with "H" and "B". As seen in the output above, full names are not consistent with the naming scheme of the other categories (A and W). I will make the assumption that "hispanic" points to the same category as "H", and thus categorize "hispanic" into "H", and same logic with "black" and "B".
```{r}
working_df <- working_df %>% 
  mutate(race = recode(race,"hispanic"="H","black"="B"))
```
Moving to gender, I believe that a similar story is occuring based on our previous analysis. Specifically, "male" and "M" are pointing to the same category, and "female" and "F" are pointing to the same category. Therefore, in the same line of thought as before, I will recode the data accordingly.
```{r}
working_df <- working_df %>% 
  mutate(gender = recode(gender,"male"="M","female"="F"))
```
In a similar line of thought, I believe that for the sports data, "0" means does not play sports, while "1" indicates the individual plays sports. Therefore, I believe that all the names of sports (basketball etc.) can be categorized as "1", since they do indeed play sports, its just that they have indicated a particular sport, and thus it is not counted as the category "1" -- play sports. Therefore, I will recode the data accordingly. Notice however, that the data type of the sports column is "chr", which means that I will have to recode into the charactor "1" first, and the change the column to a more convenient data type such as integer.
```{r}
working_df <- working_df %>% 
  mutate(plays_sports = recode(plays_sports,"soccer"="1","basketball"="1","baseball"="1","gymnastics"="1", "volleyball"="1","tennis"="1")) %>% 
  mutate()

working_df$plays_sports <- as.integer(working_df$plays_sports)
```
For the lab variable, recall from the previous section that, assuming that this variable indicates whether or not the individual goes to the lab school, it is completely reasonable to expect a ${0,1}$ dummy variable. However, we see multiple -1 values and 3 values. Therefore, I will find how many -1 and 3s there are and determine their importance on our data.
```{r}
table(working_df$lab)
cat('the number of non {0,1} values are: ', sum(working_df$lab >1 | working_df$lab <0), '\n')
cat('the number of {0,1} values are: ', sum(working_df$lab == 1 | working_df$lab == 0), '\n')
ratio_lab <- sum(working_df$lab >1 | working_df$lab <0) / sum(working_df$lab)
cat(ratio_lab, 'is the percent of all the lab data which contain non {0,1} values')
```
Because I cannot make sense of these -1 and 3s and would not like to introduce bias into our analysis, I will remove these rows. I recongnize that I am trading off robustness in the data for a more manageable interpretation, but I believe that removing these values (which might get dropped anyways due to the duplicated IDs) will give us a significantly more interpretable result when doing regressions.

Moving onto income bucket, recall from before that we have -99 values in our income data. If this income bucket was created using the income data, then there would be no real point in cleaning this data as it is inherently faulty. If we were to ever use this data, I will create a new income bucket using the original bins. Therefore, I will remove this incorrect data.
```{r}
working_df <- working_df %>% 
  select(-income_bucket)
```


#### Step 2.3: Dealing with IDs

Now I will deal with the ID column, which will be important because we do not want to double count, assuming each ID should be unique (since we are not dealing with panel data accross time). As mentioned in our data exploration phase, we note that there are duplicates in the data. Based on this understanding of the data, I will do the following:
  + delete completely identical rows, since they represent double counted data.
  + delete rows with duplicated ids yet extremely faulty information in one specific category (i.e. for two ID entries, all else the same, one has a -99 in the income category; then delete the one with the -99 in the income category). This is because the non-faulty row contains all the relevant information in these very narrow cases, where as the row which **only** differs with a -99 value will not be useful for our analysis.
  + to carefully the rows with duplicated ids, but both has very 'believable' data. This is the most tricky part, and I will deal with these on a case-by-case basis should they arise.
  
First, note what we are aiming for, ie, the true count of unique ids:
```{r}
n_distinct(working_df$student_id)
```

```{r}
#Original: 10,000 obs

#first step, getting rid of complete duplicates
working_df <- working_df %>% 
  distinct()
#6115 obs

#second step, getting rid of duplicates in ids with bad income data
working_df <- working_df %>% 
  distinct(student_id,was_neg99, .keep_all=TRUE) %>% 
  filter(was_neg99 == 0)
#5000 obs, which agrees with our theoretical 'unique' data by student ID

unique(working_df$was_neg99)
#returns 0, therefore...

working_df <- working_df %>% 
  select(-was_neg99)
```
Here, we have identified 3885 completely identical rows (ID or otherwise), and removed them accordingly.

Then, I found all the distinct id-negative_99 pairs. This is to see if we can eliminate the rest of the repeated rows by considering instances where we had garbage data (ie. -99s). As a result, by filtering out garbage data, we were able to directly arrive at our desired state -- where we have all unique IDs! Therefore, I will not need to do the third step in our operation (looking at individual cases), and I am also able to remove the was_neg99 column, since it no longer has any meaning in our analysis (since all rows originally with -99s are gone). From here, I will move forward with Q3.

### Question 3: 
With your cleaned data, re-estimate and report the treatment effects for the two methods in question 1, assuming the randomization is valid. What are the differences in results between the two methods?  Do the estimates imply a big or small impact of the school?  Positive or negative? Is one estimate more convincing than the other? 

#### Step 1: Allocating for robustness check and exploring correlations
Before doing regressions, lets consider the outliers which might affect the results of estimating the effect on the treatment effect on grades by lab school education. Recall that all the continuous variables which might seek to explain current scores (so: income, distance, past scores) are normally distirbuted. Here, I will cut off the tails of each distribution and save them in a separate dataframe to and run a regression on each to determine the initial results. Then, I will use each resultant regression on the full data set to check for the robustness of the result and interpret from there.

```{r}
cutoffs_inc <- quantile(working_df$income,c(0.05,0.95)) 
cutoffs_dist <- quantile(working_df$dist,c(0.05,0.95))
cutoffs_pscore <- quantile(working_df$past_score,c(0.05,0.95))

tiny_inc_df <- working_df %>% 
  filter(between(income,cutoffs_inc[1],cutoffs_inc[2]))

tiny_dist_df <- working_df %>% 
  filter(between(dist,cutoffs_dist[1],cutoffs_dist[2]))

tiny_pscore_df <- working_df %>% 
  filter(between(past_score,cutoffs_pscore[1],cutoffs_pscore[2]))
```

Now I will use a correlation matrix to check the correlations between each of the continuous variables from each of the tiny dataframes created above. In doing so, I can get a rough idea as to each variables correlate with each other in the cleaned dataframe, and discuss potential regressions from there.

```{r}
library(corrplot)

ctn_only_tinyinc <- tiny_inc_df %>% 
  select(income,dist,score,past_score)

ctn_only_tinydist <- tiny_dist_df %>% 
  select(income,dist,score,past_score)

ctn_only_tinypscore <- tiny_pscore_df %>% 
  select(income,dist,score,past_score)

ctn_only_df <- working_df %>% 
  select(income,dist,score,past_score)

corrplot.mixed(cor(ctn_only_tinyinc),title = "Tiny Income", mar=c(0,0,1,0))
corrplot.mixed(cor(ctn_only_tinydist),title = "Tiny Distance", mar=c(0,0,1,0))
corrplot.mixed(cor(ctn_only_tinypscore),title = "Tiny Past Score", mar=c(0,0,1,0))
corrplot.mixed(cor(ctn_only_df),title = "Tiny DF", mar=c(0,0,1,0))
```
Notice that each of the tiny dataframes gives very slightly different results. It is quite interesting that removing outliers for any of the aforementioned variables in the dataframe will generally decrease the correlations between the variables. However, this is a trade-off that I believe will be helpful in producing a more concrete analysis because we would not want outliers to skew our results. With everything ready, I will move forward with regressions.

### Step 2: Producing Regressions 

#### Step 2.1: Replicating the no-covariate regression with different tinys.

```{r}
nocov_reg_inc <- lm(data = tiny_inc_df, score ~ lab)
nocov_reg_dist <- lm(data = tiny_dist_df, score ~ lab)
nocov_reg_pscore <- lm(data = tiny_pscore_df, score ~ lab)
nocov_reg_fulldf <- lm(data = working_df, score ~ lab)

summary(nocov_reg_inc)
summary(nocov_reg_dist)
summary(nocov_reg_pscore)
summary(nocov_reg_fulldf)
```
Here, we have produced 4 regressions, the first three consists of removing outliers of particular variables, where the last is the full clean dataframe. In all of the new regressions, we have much more interpretable and believeable results. Furthermore, it is very interesting that removing outliers in our data (regardless of category), strengthens both the economic significance (larger magnitude of coefficient) as well as statistical significance (p-values) of our regression. To address the question at hand, I will move forward with the model with covariates and compare the simple model above with the final model with cerntain covariates.

#### Step 2.2: Replicating 
Now adding more covariates into the model, we must consider what should be the proper variables to consider. I believe the first variable to consider is past scores. This is because of the following reasons:
 + Theoretically, I believe that one's ability to take previous exams should serve as an indication for their ability to take future exams. This is because their overall 'competency' for taking the same exam over time should be more or less consistent, i.e. someone not that great at taking the standardized test wouldn't be an awesome exam taker out of the blue. On the other hand, a great exam taker has no reason to suddenly become terrible with standardized tests. Therefore, not accounting for these situations in our model would skew our conclusion on the effectiveness of attending the lab school.
 + Emperically, the correlation between the two scores are quite high, and this high correlation which indicates how the two columns are interrelated should be taken into account when constructing a model. Else the error term on the regression would be quite high.
 
Now that I have argued for the inclusion of the past scores, I would also like to argue that 